---
layout: post
title: "Snow and Ice Removal Analysis"
author: Ken Cavagnolo
category : ipynotebooks
tags: [python, notebook, jupyter]
comments: true
tweets: true
---

**In [1]:**

{% highlight python %}
# partial imports
from __future__ import division
import warnings
warnings.filterwarnings('ignore')
from tqdm import *
from pymc import MCMC
from math import radians, cos, sin, asin, sqrt
from sqlalchemy import create_engine
from operator import itemgetter

# full imports
import os
import sys
import time
import datetime
import urllib2
import requests
import operator
import pandas as pd
import numpy as np
import json

# plotly
import plotly.plotly as py
import plotly.tools as tls
from plotly.graph_objs import *
tls.set_credentials_file(username='kcavagnolo', api_key='**********')

#matplotlib
import matplotlib.pyplot as plt
%matplotlib inline
{% endhighlight %}

**In [2]:**

{% highlight python %}
# define ODOT data location
datadir = '/home/kcavagnolo/odot/data/odot_ops/'
raw_equip = datadir+'FY15_Snow_and_Ice_Equipment.xlsx'
raw_labor = datadir+'FY15_Snow_and_Ice_Labor.xlsx'
raw_mater = datadir+'FY15_Snow_and_Ice_Materials.xlsx'
raw_assoc = datadir+'FY15_Snow_and_Ice_Outpost_Associations.xlsx'
{% endhighlight %}

**In [3]:**

{% highlight python %}
# read in data
equip = pd.ExcelFile(raw_equip)
labor = pd.ExcelFile(raw_labor)
mater = pd.ExcelFile(raw_mater)
assoc = pd.ExcelFile(raw_assoc)
{% endhighlight %}

**In [4]:**

{% highlight python %}
# show me how many sheets each file has in it
snames = [equip.sheet_names,
          labor.sheet_names,
          mater.sheet_names,
          assoc.sheet_names]
for n in snames:
    print n
{% endhighlight %}

    [u'FY15 Snow and Ice Equipment']
    [u'FY15 Snow and Ice Labor']
    [u'FY15 Snow and Ice Materials']
    [u'Abb Lookup']


**In [5]:**

{% highlight python %}
# keep track of all df's
dfs = []
dfs_names = []

# in luck, one sheet per df, so read only sheet in each df 
df_equip = equip.parse(equip.sheet_names[0])
dfs.append(df_equip)
dfs_names.append('equipment')

df_labor = labor.parse(labor.sheet_names[0])
dfs.append(df_labor)
dfs_names.append('labor')

df_mater = mater.parse(mater.sheet_names[0])
dfs.append(df_mater)
dfs_names.append('material')

df_assoc = assoc.parse(assoc.sheet_names[0])
dfs.append(df_assoc)
dfs_names.append('associations')

# tell me what's in each df
for i, df in enumerate(dfs):
    nulls = np.count_nonzero(df.isnull())
    print '{:15}'.format(snames[i])
    print '{:15} {:d}'.format('Observations:', df.shape[0])
    print '{:15} {:d}'.format('Features:', df.shape[1])
    print '{:15} {:d}'.format('Nulls:', nulls)
    print '\n------------------------------------------\n'
{% endhighlight %}

    [u'FY15 Snow and Ice Equipment']
    Observations:   98595
    Features:       12
    Nulls:          5001
    
    ------------------------------------------
    
    [u'FY15 Snow and Ice Labor']
    Observations:   190430
    Features:       10
    Nulls:          52
    
    ------------------------------------------
    
    [u'FY15 Snow and Ice Materials']
    Observations:   34097
    Features:       11
    Nulls:          8
    
    ------------------------------------------
    
    [u'Abb Lookup']
    Observations:   226
    Features:       3
    Nulls:          0
    
    ------------------------------------------
    


### We have a NULL's problem. Let's see where they are...

**In [6]:**

{% highlight python %}
for i, df in enumerate(dfs):
    print snames[i]
    print df.describe()
    print "\n---------------NULL's--------------------------\n"
    print df.isnull().sum()
    print "\n-----------------------------------------------\n"
{% endhighlight %}

    [u'FY15 Snow and Ice Equipment']
           Work Fiscal Year    Work Order #    Equipment #    Meter Usage  \
    count             98595    98595.000000    98595.000000  94203.000000   
    mean               2015  4158162.738638  2822160.527714    139.840484   
    std                   0   161839.384687  1037552.966006    130.480380   
    min                2015  3630426.000000  1017474.000000      0.000000   
    25%                2015  4022831.000000  2544462.000000     62.000000   
    50%                2015  4179180.000000  2545872.000000    118.000000   
    75%                2015  4231463.000000  2560707.000000    192.000000   
    max                2015  5233178.000000  9400436.000000  17412.000000   
    
           Hours on Job     Unit Cost  Equipment Cost  
    count  98593.000000  98003.000000    98580.000000  
    mean       8.563609      5.703744      374.390999  
    std        4.970066     10.487427      350.799502  
    min        0.100000      0.000000        0.000000  
    25%        5.000000      2.520000      151.200000  
    50%        8.000000      2.520000      317.220000  
    75%       12.000000      3.110000      522.480000  
    max       24.000000    130.300000    43878.240000  
    
    ---------------NULL's--------------------------
    
    Activity                  0
    Owner                     0
    Work Fiscal Year          0
    Work Date                 0
    Work Order #              0
    Equipment #               0
    Equipment Category        0
    Meter1 Type               0
    Meter Usage            4392
    Hours on Job              2
    Unit Cost               592
    Equipment Cost           15
    dtype: int64
    
    -----------------------------------------------
    
    [u'FY15 Snow and Ice Labor']
             Work Order #  Work Fiscal Year    Total Hours    Hourly Rate  \
    count   190430.000000            190430  190430.000000  190404.000000   
    mean   4160107.686305              2015       6.131547      18.835078   
    std     165426.383568                 0       2.896111       3.710014   
    min    3630426.000000              2015       0.100000       0.000000   
    25%    4023122.000000              2015       4.000000      16.220000   
    50%    4176510.000000              2015       7.000000      18.280000   
    75%    4231416.000000              2015       8.000000      19.970000   
    max    5459840.000000              2015      24.000000      66.237500   
    
              Labor Cost  
    count  190404.000000  
    mean      114.733908  
    std        58.718097  
    min         0.000000  
    25%        70.280000  
    50%       123.280000  
    75%       151.360000  
    max       711.109000  
    
    ---------------NULL's--------------------------
    
    Activity             0
    Owner                0
    Work Date            0
    Work Order #         0
    Employee             0
    Work Fiscal Year     0
    TRC/Paycode          0
    Total Hours          0
    Hourly Rate         26
    Labor Cost          26
    dtype: int64
    
    -----------------------------------------------
    
    [u'FY15 Snow and Ice Materials']
           Work Fiscal Year    Work Order #        Amount     Unit Cost  \
    count             34097    34097.000000  34097.000000  34093.000000   
    mean               2015  4134109.355720    354.218383     36.962582   
    std                   0   143338.411216   1628.195749     64.576049   
    min                2015  3644160.000000      0.000000    -39.007200   
    25%                2015  3991978.000000      9.000000      0.726500   
    50%                2015  4163295.000000     40.000000     42.000000   
    75%                2015  4226153.000000    171.500000     64.414400   
    max                2015  5687550.000000  60096.000000   5338.520000   
    
            Direct Cost  
    count  34093.000000  
    mean    1887.270060  
    std     4218.565853  
    min    -2247.817000  
    25%       57.050000  
    50%      355.752000  
    75%     1495.008000  
    max    94671.458300  
    
    ---------------NULL's--------------------------
    
    Activity                     0
    Owner                        0
    Work Fiscal Year             0
    Work Date                    0
    Work Order #                 0
    Material Master Code         0
    Material Master Code Desc    0
    Amount                       0
    Unit Cost                    4
    Unit of Measure              0
    Direct Cost                  4
    dtype: int64
    
    -----------------------------------------------
    
    [u'Abb Lookup']
             District
    count  226.000000
    mean     6.411504
    std      3.217128
    min      1.000000
    25%      4.000000
    50%      6.000000
    75%      9.000000
    max     12.000000
    
    ---------------NULL's--------------------------
    
    District                  0
    Division / Cost Center    0
    County Abb                0
    dtype: int64
    
    -----------------------------------------------
    


**In [7]:**

{% highlight python %}
# clean-up column names and types
for df in dfs:
    df.columns = [c.lower().replace(' ', '_').replace('?', '').replace("'", "").replace("#","num") for c in df.columns]
    col_names = df.columns.tolist()
    print col_names
    print
{% endhighlight %}

    [u'activity_', u'owner', u'work_fiscal_year', u'work_date', u'work_order_num', u'equipment_num_', u'equipment_category_', u'meter1_type', u'meter_usage', u'hours_on_job', u'unit_cost', u'equipment_cost']
    
    [u'activity_', u'owner', u'work_date', u'work_order_num', u'employee', u'work_fiscal_year', u'trc/paycode_', u'total_hours', u'hourly_rate', u'labor_cost']
    
    [u'activity_', u'owner', u'work_fiscal_year', u'work_date', u'work_order_num', u'material_master_code', u'material_master_code_desc', u'amount', u'unit_cost', u'unit_of_measure', u'direct_cost']
    
    [u'district', u'division_/_cost_center', u'county_abb']
    


**In [8]:**

{% highlight python %}
df_labor.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>activity_</th>
      <th>owner</th>
      <th>work_date</th>
      <th>work_order_num</th>
      <th>employee</th>
      <th>work_fiscal_year</th>
      <th>trc/paycode_</th>
      <th>total_hours</th>
      <th>hourly_rate</th>
      <th>labor_cost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td>       ENGLE, BRANT J</td>
      <td> 2015</td>
      <td> REG - REGLR Regular Pay</td>
      <td> 8.0</td>
      <td> 18.20</td>
      <td> 145.60</td>
    </tr>
    <tr>
      <th>1</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td>       MILLER, ERIC D</td>
      <td> 2015</td>
      <td> REG - REGLR Regular Pay</td>
      <td> 8.0</td>
      <td> 17.72</td>
      <td> 141.76</td>
    </tr>
    <tr>
      <th>2</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td>     PLACE, JEFFREY J</td>
      <td> 2015</td>
      <td> REG - REGLR Regular Pay</td>
      <td> 6.8</td>
      <td> 21.05</td>
      <td> 143.14</td>
    </tr>
    <tr>
      <th>3</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td>     KAUFMAN, LARRY J</td>
      <td> 2015</td>
      <td> REG - REGLR Regular Pay</td>
      <td> 8.0</td>
      <td> 17.22</td>
      <td> 137.76</td>
    </tr>
    <tr>
      <th>4</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> HERMILLER, KENNETH L</td>
      <td> 2015</td>
      <td> REG - REGLR Regular Pay</td>
      <td> 8.0</td>
      <td> 19.33</td>
      <td> 154.64</td>
    </tr>
  </tbody>
</table>
</div>



**In [9]:**

{% highlight python %}
df_labor["activity_"] = df_labor["activity_"].str.lower()
df_labor["act_id"], df_labor["act_name"] = zip(*df_labor["activity_"].str.split(' - ').tolist())
del(df_labor["activity_"])

df_labor["owner"] = df_labor["owner"].str.lower()
df_labor["junk"], df_labor["cost_center_name"] = zip(*df_labor["owner"].str.split(' - ').tolist())
del(df_labor["owner"])

df_labor["division_id"], df_labor["cost_center_id"] = zip(*df_labor["junk"].str.split(' ').tolist())
del(df_labor["junk"])

df_labor["employee"] = df_labor["employee"].str.lower()
df_labor["emp_lname"], df_labor["rmp_fname"] = zip(*df_labor["employee"].str.split(', ').tolist())
del(df_labor["employee"])

df_labor["trc/paycode_"] = df_labor["trc/paycode_"].str.lower()
df_labor["trc"], df_labor["paycode"] = zip(*df_labor["trc/paycode_"].str.split(' - ').tolist())
del(df_labor["trc/paycode_"])
{% endhighlight %}

**In [10]:**

{% highlight python %}
df_labor.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>work_date</th>
      <th>work_order_num</th>
      <th>work_fiscal_year</th>
      <th>total_hours</th>
      <th>hourly_rate</th>
      <th>labor_cost</th>
      <th>act_id</th>
      <th>act_name</th>
      <th>cost_center_name</th>
      <th>division_id</th>
      <th>cost_center_id</th>
      <th>emp_lname</th>
      <th>rmp_fname</th>
      <th>trc</th>
      <th>paycode</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2015</td>
      <td> 8.0</td>
      <td> 18.20</td>
      <td> 145.60</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td>     engle</td>
      <td>   brant j</td>
      <td> reg</td>
      <td> reglr regular pay</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2015</td>
      <td> 8.0</td>
      <td> 17.72</td>
      <td> 141.76</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td>    miller</td>
      <td>    eric d</td>
      <td> reg</td>
      <td> reglr regular pay</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2015</td>
      <td> 6.8</td>
      <td> 21.05</td>
      <td> 143.14</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td>     place</td>
      <td> jeffrey j</td>
      <td> reg</td>
      <td> reglr regular pay</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2015</td>
      <td> 8.0</td>
      <td> 17.22</td>
      <td> 137.76</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td>   kaufman</td>
      <td>   larry j</td>
      <td> reg</td>
      <td> reglr regular pay</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2015</td>
      <td> 8.0</td>
      <td> 19.33</td>
      <td> 154.64</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td> hermiller</td>
      <td> kenneth l</td>
      <td> reg</td>
      <td> reglr regular pay</td>
    </tr>
  </tbody>
</table>
</div>



**In [11]:**

{% highlight python %}
df_equip.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>activity_</th>
      <th>owner</th>
      <th>work_fiscal_year</th>
      <th>work_date</th>
      <th>work_order_num</th>
      <th>equipment_num_</th>
      <th>equipment_category_</th>
      <th>meter1_type</th>
      <th>meter_usage</th>
      <th>hours_on_job</th>
      <th>unit_cost</th>
      <th>equipment_cost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2130510</td>
      <td>            213 - UTILITY TRUCK 1 TON</td>
      <td> MILES</td>
      <td> 69.0</td>
      <td> 1</td>
      <td>  0.83</td>
      <td> 57.27</td>
    </tr>
    <tr>
      <th>1</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 5910086</td>
      <td>             591 - LOADER, SKID STEER</td>
      <td> HOURS</td>
      <td>  5.5</td>
      <td> 1</td>
      <td> 22.00</td>
      <td> 22.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2340174</td>
      <td> 234 - STAKE, 1 1/2 &amp; 2 TON, W/LFTGTE</td>
      <td> MILES</td>
      <td> 41.0</td>
      <td> 1</td>
      <td>  2.13</td>
      <td> 87.33</td>
    </tr>
    <tr>
      <th>3</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2330266</td>
      <td>  233 - STAKE, 1 1/2 &amp; OVER, STANDARD</td>
      <td> MILES</td>
      <td> 24.0</td>
      <td> 1</td>
      <td>  2.05</td>
      <td> 49.20</td>
    </tr>
    <tr>
      <th>4</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-01-08</td>
      <td> 4162034</td>
      <td> 2130510</td>
      <td>            213 - UTILITY TRUCK 1 TON</td>
      <td> MILES</td>
      <td> 26.0</td>
      <td> 1</td>
      <td>  0.83</td>
      <td> 21.58</td>
    </tr>
  </tbody>
</table>
</div>



**In [12]:**

{% highlight python %}
field = "activity_"
splitter = ' - '
nfields = ["act_id", "act_name"]
df_equip[field] = df_equip[field].str.lower()
df_equip[nfields[0]], df_equip[nfields[1]] = zip(*df_equip[field].str.split(splitter).tolist())
del(df_equip[field])

field = "owner"
splitter = ' - '
nfields = ["junk", "cost_center_name"]
df_equip[field] = df_equip[field].str.lower()
df_equip[nfields[0]], df_equip[nfields[1]] = zip(*df_equip[field].str.split(splitter).tolist())
del(df_equip[field])

field = "junk"
splitter = ' '
nfields = ["division_id", "cost_center_id"]
df_equip[field] = df_equip[field].str.lower()
df_equip[nfields[0]], df_equip[nfields[1]] = zip(*df_equip[field].str.split(splitter).tolist())
del(df_equip[field])

field = "equipment_category_"
splitter = ' - '
nfields = ["equip_id", "equip_name"]
df_equip[field] = df_equip[field].str.lower()
df_equip[field] = df_equip[field].str.replace("&", " ")
df_equip[field] = df_equip[field].str.replace(",", " ")
df_equip[nfields[0]], df_equip[nfields[1]] = zip(*df_equip[field].str.split(splitter).tolist())
del(df_equip[field])
{% endhighlight %}

**In [13]:**

{% highlight python %}
df_equip.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>work_fiscal_year</th>
      <th>work_date</th>
      <th>work_order_num</th>
      <th>equipment_num_</th>
      <th>meter1_type</th>
      <th>meter_usage</th>
      <th>hours_on_job</th>
      <th>unit_cost</th>
      <th>equipment_cost</th>
      <th>act_id</th>
      <th>act_name</th>
      <th>cost_center_name</th>
      <th>division_id</th>
      <th>cost_center_id</th>
      <th>equip_id</th>
      <th>equip_name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 2015</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2130510</td>
      <td> MILES</td>
      <td> 69.0</td>
      <td> 1</td>
      <td>  0.83</td>
      <td> 57.27</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td> 213</td>
      <td>            utility truck 1 ton</td>
    </tr>
    <tr>
      <th>1</th>
      <td> 2015</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 5910086</td>
      <td> HOURS</td>
      <td>  5.5</td>
      <td> 1</td>
      <td> 22.00</td>
      <td> 22.00</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td> 591</td>
      <td>             loader  skid steer</td>
    </tr>
    <tr>
      <th>2</th>
      <td> 2015</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2340174</td>
      <td> MILES</td>
      <td> 41.0</td>
      <td> 1</td>
      <td>  2.13</td>
      <td> 87.33</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td> 234</td>
      <td> stake  1 1/2   2 ton  w/lftgte</td>
    </tr>
    <tr>
      <th>3</th>
      <td> 2015</td>
      <td>2015-01-07</td>
      <td> 4162034</td>
      <td> 2330266</td>
      <td> MILES</td>
      <td> 24.0</td>
      <td> 1</td>
      <td>  2.05</td>
      <td> 49.20</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td> 233</td>
      <td>  stake  1 1/2   over  standard</td>
    </tr>
    <tr>
      <th>4</th>
      <td> 2015</td>
      <td>2015-01-08</td>
      <td> 4162034</td>
      <td> 2130510</td>
      <td> MILES</td>
      <td> 26.0</td>
      <td> 1</td>
      <td>  0.83</td>
      <td> 21.58</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
      <td> 213</td>
      <td>            utility truck 1 ton</td>
    </tr>
  </tbody>
</table>
</div>



**In [14]:**

{% highlight python %}
df_mater.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>activity_</th>
      <th>owner</th>
      <th>work_fiscal_year</th>
      <th>work_date</th>
      <th>work_order_num</th>
      <th>material_master_code</th>
      <th>material_master_code_desc</th>
      <th>amount</th>
      <th>unit_cost</th>
      <th>unit_of_measure</th>
      <th>direct_cost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-01-13</td>
      <td> 4172938</td>
      <td> # 42010005 SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td> SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td>  6</td>
      <td> 4.36</td>
      <td> Each</td>
      <td> 26.16</td>
    </tr>
    <tr>
      <th>1</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-01-14</td>
      <td> 4172938</td>
      <td> # 42010005 SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td> SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td> 10</td>
      <td> 4.36</td>
      <td> Each</td>
      <td> 43.60</td>
    </tr>
    <tr>
      <th>2</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-01-22</td>
      <td> 4172938</td>
      <td> # 42010005 SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td> SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td> 12</td>
      <td> 4.36</td>
      <td> Each</td>
      <td> 52.32</td>
    </tr>
    <tr>
      <th>3</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-02-03</td>
      <td> 4193442</td>
      <td> # 42010005 SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td> SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td>  7</td>
      <td> 0.44</td>
      <td> Each</td>
      <td>  3.08</td>
    </tr>
    <tr>
      <th>4</th>
      <td> M690-001 - Snow and Ice</td>
      <td> 0001 5400 - Roadway Services</td>
      <td> 2015</td>
      <td>2015-02-04</td>
      <td> 4193442</td>
      <td> # 42010005 SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td> SALT, WINTER MELT ROCK SALT 50#BAG</td>
      <td> 17</td>
      <td> 0.44</td>
      <td> Each</td>
      <td>  7.48</td>
    </tr>
  </tbody>
</table>
</div>



**In [15]:**

{% highlight python %}
field = "activity_"
splitter = ' - '
nfields = ["act_id", "act_name"]
df_mater[field] = df_mater[field].str.lower()
df_mater[nfields[0]], df_mater[nfields[1]] = zip(*df_mater[field].str.split(splitter).tolist())
del(df_mater[field])

field = "owner"
splitter = ' - '
nfields = ["junk", "cost_center_name"]
df_mater[field] = df_mater[field].str.lower()
df_mater[nfields[0]], df_mater[nfields[1]] = zip(*df_mater[field].str.split(splitter).tolist())
del(df_mater[field])

field = "junk"
splitter = ' '
nfields = ["division_id", "cost_center_id"]
df_mater[field] = df_mater[field].str.lower()
df_mater[nfields[0]], df_mater[nfields[1]] = zip(*df_mater[field].str.split(splitter).tolist())
del(df_mater[field])

mcode = []
for n in df_mater["material_master_code"].str.split().tolist():
    mcode.append(n[1])
df_mater["material_master_code"] = mcode

field = "material_master_code_desc"
df_mater[field] = df_mater[field].str.replace('#',"lb")
df_mater[field] = df_mater[field].str.lower()
{% endhighlight %}

**In [16]:**

{% highlight python %}
df_mater.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>work_fiscal_year</th>
      <th>work_date</th>
      <th>work_order_num</th>
      <th>material_master_code</th>
      <th>material_master_code_desc</th>
      <th>amount</th>
      <th>unit_cost</th>
      <th>unit_of_measure</th>
      <th>direct_cost</th>
      <th>act_id</th>
      <th>act_name</th>
      <th>cost_center_name</th>
      <th>division_id</th>
      <th>cost_center_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 2015</td>
      <td>2015-01-13</td>
      <td> 4172938</td>
      <td> 42010005</td>
      <td> salt, winter melt rock salt 50lbbag</td>
      <td>  6</td>
      <td> 4.36</td>
      <td> Each</td>
      <td> 26.16</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
    </tr>
    <tr>
      <th>1</th>
      <td> 2015</td>
      <td>2015-01-14</td>
      <td> 4172938</td>
      <td> 42010005</td>
      <td> salt, winter melt rock salt 50lbbag</td>
      <td> 10</td>
      <td> 4.36</td>
      <td> Each</td>
      <td> 43.60</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
    </tr>
    <tr>
      <th>2</th>
      <td> 2015</td>
      <td>2015-01-22</td>
      <td> 4172938</td>
      <td> 42010005</td>
      <td> salt, winter melt rock salt 50lbbag</td>
      <td> 12</td>
      <td> 4.36</td>
      <td> Each</td>
      <td> 52.32</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
    </tr>
    <tr>
      <th>3</th>
      <td> 2015</td>
      <td>2015-02-03</td>
      <td> 4193442</td>
      <td> 42010005</td>
      <td> salt, winter melt rock salt 50lbbag</td>
      <td>  7</td>
      <td> 0.44</td>
      <td> Each</td>
      <td>  3.08</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
    </tr>
    <tr>
      <th>4</th>
      <td> 2015</td>
      <td>2015-02-04</td>
      <td> 4193442</td>
      <td> 42010005</td>
      <td> salt, winter melt rock salt 50lbbag</td>
      <td> 17</td>
      <td> 0.44</td>
      <td> Each</td>
      <td>  7.48</td>
      <td> m690-001</td>
      <td> snow and ice</td>
      <td> roadway services</td>
      <td> 0001</td>
      <td> 5400</td>
    </tr>
  </tbody>
</table>
</div>



**In [17]:**

{% highlight python %}
df_assoc.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>district</th>
      <th>division_/_cost_center</th>
      <th>county_abb</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 1</td>
      <td>    0001 6100 - Allen County Garage</td>
      <td> ALL</td>
    </tr>
    <tr>
      <th>1</th>
      <td> 1</td>
      <td>     0001 6101 - 4th Street Outpost</td>
      <td> ALL</td>
    </tr>
    <tr>
      <th>2</th>
      <td> 1</td>
      <td>      0001 6102 - Beaverdam Outpost</td>
      <td> ALL</td>
    </tr>
    <tr>
      <th>3</th>
      <td> 1</td>
      <td>        0001 6103 - Delphos Outpost</td>
      <td> ALL</td>
    </tr>
    <tr>
      <th>4</th>
      <td> 1</td>
      <td> 0001 6200 - Defiance County Garage</td>
      <td> DEF</td>
    </tr>
  </tbody>
</table>
</div>



**In [18]:**

{% highlight python %}
field = "division_/_cost_center"
splitter = ' - '
nfields = ["division", "cost_center"]
df_assoc[field] = df_assoc[field].str.lower()
df_assoc[nfields[0]], df_assoc[nfields[1]] = zip(*df_assoc[field].str.split(splitter).tolist())
del(df_assoc[field])

df_assoc["division_id"], df_assoc["cost_center_id"] = zip(*df_assoc["division"].str.split(" ").tolist())
del(df_assoc["division"])

df_assoc["county_abb"] = df_assoc["county_abb"].str.lower()
{% endhighlight %}

**In [19]:**

{% highlight python %}
df_assoc.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>district</th>
      <th>county_abb</th>
      <th>cost_center</th>
      <th>division_id</th>
      <th>cost_center_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 1</td>
      <td> all</td>
      <td>    allen county garage</td>
      <td> 0001</td>
      <td> 6100</td>
    </tr>
    <tr>
      <th>1</th>
      <td> 1</td>
      <td> all</td>
      <td>     4th street outpost</td>
      <td> 0001</td>
      <td> 6101</td>
    </tr>
    <tr>
      <th>2</th>
      <td> 1</td>
      <td> all</td>
      <td>      beaverdam outpost</td>
      <td> 0001</td>
      <td> 6102</td>
    </tr>
    <tr>
      <th>3</th>
      <td> 1</td>
      <td> all</td>
      <td>        delphos outpost</td>
      <td> 0001</td>
      <td> 6103</td>
    </tr>
    <tr>
      <th>4</th>
      <td> 1</td>
      <td> def</td>
      <td> defiance county garage</td>
      <td> 0001</td>
      <td> 6200</td>
    </tr>
  </tbody>
</table>
</div>



**In [20]:**

{% highlight python %}
df_assoc["division_id"].unique()
{% endhighlight %}




    array([u'0001', u'0002', u'0003', u'0004', u'0005', u'0006', u'0007',
           u'0008', u'0009', u'0010', u'0011', u'0012'], dtype=object)



**In [21]:**

{% highlight python %}
ccid = sorted(df_assoc["cost_center_id"].unique().tolist())
l = df_labor["cost_center_id"].unique().tolist()
m = df_mater["cost_center_id"].unique().tolist()
e = df_equip["cost_center_id"].unique().tolist()
a = l+m+e
for n in list(set(a)):
    if n not in ccid:
        print n, "missing from ccid "
        
did = sorted(df_assoc["division_id"].unique().tolist())
l = df_labor["division_id"].unique().tolist()
m = df_mater["division_id"].unique().tolist()
e = df_equip["division_id"].unique().tolist()
a = l+m+e
for n in list(set(a)):
    if n not in did:
        print n, "missing from did "
{% endhighlight %}

    5440 missing from ccid 
    6111 missing from ccid 
    5430 missing from ccid 
    5410 missing from ccid 
    5400 missing from ccid 
    5420 missing from ccid 


**In [22]:**

{% highlight python %}
# clean-up column names and types
for df in dfs:
    for c in df.columns.tolist():
        testval = df[c][0]
        if isinstance(testval, unicode):
            if testval.isdigit():
                print 'Fix? ', c, type(testval), " to int"
                #df[c] = df[c].astype(int)
    print
{% endhighlight %}

    Fix?  division_id <type 'unicode'>  to int
    Fix?  cost_center_id <type 'unicode'>  to int
    Fix?  equip_id <type 'unicode'>  to int
    
    Fix?  division_id <type 'unicode'>  to int
    Fix?  cost_center_id <type 'unicode'>  to int
    
    Fix?  material_master_code <type 'unicode'>  to int
    Fix?  division_id <type 'unicode'>  to int
    Fix?  cost_center_id <type 'unicode'>  to int
    
    Fix?  division_id <type 'unicode'>  to int
    Fix?  cost_center_id <type 'unicode'>  to int
    


I want to see what the cross-plots look like

**In [23]:**

{% highlight python %}
# what's in the df's
for df in dfs:
    for c in sorted(df.columns.tolist()):
        print c
    print "\n-----------------------------------------------\n"
{% endhighlight %}

    act_id
    act_name
    cost_center_id
    cost_center_name
    division_id
    equip_id
    equip_name
    equipment_cost
    equipment_num_
    hours_on_job
    meter1_type
    meter_usage
    unit_cost
    work_date
    work_fiscal_year
    work_order_num
    
    -----------------------------------------------
    
    act_id
    act_name
    cost_center_id
    cost_center_name
    division_id
    emp_lname
    hourly_rate
    labor_cost
    paycode
    rmp_fname
    total_hours
    trc
    work_date
    work_fiscal_year
    work_order_num
    
    -----------------------------------------------
    
    act_id
    act_name
    amount
    cost_center_id
    cost_center_name
    direct_cost
    division_id
    material_master_code
    material_master_code_desc
    unit_cost
    unit_of_measure
    work_date
    work_fiscal_year
    work_order_num
    
    -----------------------------------------------
    
    cost_center
    cost_center_id
    county_abb
    district
    division_id
    
    -----------------------------------------------
    


Plots are failing, fields in dataframes are not int, float, or string. What are they?

**In [None]:**

{% highlight python %}
# don't run, not working
#import seaborn as sns
#sns.pairplot(data=df_equip[["act_id",
#                            "equip_id",
#                            "hours_on_job",
#                            "division_id",
#                            "unit_cost"]],
#             hue="hours_on_job", dropna=True)
#plt.show()
{% endhighlight %}

All clean. Can we join these dataframes in some meaningful way? Moving it into PSQL

**In [None]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')

# write the dataframes to psql
for i, df in enumerate(dfs):
    df.to_sql(dfs_names[i], engine, if_exists='replace', index=False)
engine.dispose()
{% endhighlight %}

## County Aggregation

Need to get all the MEL tables to be associated with a county. First need to have county names in the associations table.

**In [None]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')
conn = engine.connect()

# while I'm at it, add county name to associations table
tname = 'associations'
query = []
query.append("ALTER TABLE {0} ADD COLUMN county text DEFAULT '';")
query.append("UPDATE {0} AS a SET county=c.name_2 FROM usa_adm2 c WHERE c.name_1='Ohio' and substring(lower(c.name_2),1,3) = a.county_abb;")
query.append('''UPDATE {0} SET county='Ashtabula' WHERE county_abb='atb';
           UPDATE {0} SET county='Harrison' WHERE county_abb='has';
           UPDATE {0} SET county='Champaign' WHERE county_abb='chp';
           UPDATE {0} SET county='Monroe' WHERE county_abb='moe';
           UPDATE {0} SET county='Montgomery' WHERE county_abb='mot';
           UPDATE {0} SET county='Morgan' WHERE county_abb='mrg';
           UPDATE {0} SET county='Morrow' WHERE county_abb='mrw';
           UPDATE {0} SET county='Meigs' WHERE county_abb='meg';
           ''')

# run the queries
for q in query:
    conn.execute(q.format(tname))
    
# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

Also need to add the county name to the MEL tables on each line
```sql
alter table material add column county text default '';
update material as m set county=a.county from associations a where a.cost_center = m.cost_center_name;

alter table equipment add column county text default '';
update equipment as e set county=a.county from associations a where a.cost_center = e.cost_center_name;

alter table labor add column county text default '';
update labor as l set county=a.county from associations a where a.cost_center = l.cost_center_name;
```

To get the total spend, add everything up:

```sql
drop table summary;
create table summary as
select initcap(m.county) as county,
       m.sum as mater_sum,
       e.sum as equip_sum,
       l.sum as labor_sum,
       m.sum + e.sum + l.sum as total_sum
from (select lower(county) as county, sum(direct_cost) from material group by 1) as m
join (select lower(county) as county, sum(equipment_cost) from equipment group by 1) as e on e.county=m.county
join (select lower(county) as county, sum(labor_cost) from labor group by 1) as l on l.county=m.county;
```

Also adding the total lane miles per county:
```sql
ALTER TABLE summary DROP COLUMN IF EXISTS lane_miles;
ALTER TABLE summary ADD COLUMN lane_miles numeric DEFAULT 0.0;
UPDATE summary AS s SET lane_miles=sub1.lane_mile FROM (
    select initcap(lower(county)) as county,
           sum(segment_le) as lane_mile
    from snow_ice_priority_routes
    group by 1) as sub1
    WHERE sub1.county=s.county;
```

## Weather Events

ODOT does not have a specific definition for a weather event the calls for road pre-treatment or treatment. The closest is >50t of material used in a 24h period.

How about we let the weather guide our definition? For example:
* Rain-to-ice, sleet, or snow are expected to fall, or are falling
* In a given county, what is the relationship between material+equipment+labor (MEL) mobilization and accumulation?
* In that question are two parts:
    a. MEL consumed over a given time period per county; we have this
    b. Accumulation per county in a given time frame

To get 'b,' I'll need weather data per county per hour over the treatment season. Can scrape this from the [forecast.io API](https://developer.forecast.io). The API is accessible via a nice little python library: [python-forecastio](https://github.com/ZeevG/python-forecast.io). Let's test it out...

**In [2]:**

{% highlight python %}
import forecastio

api_key = '*****'
if len(api_key) < 32:
    raise MyError('API Key too short.')
{% endhighlight %}

**In [74]:**

{% highlight python %}
# for one day only and one location
lat = 39.759087
lng = -84.189806
date = datetime.datetime(2015,2,2)
forecast = forecastio.load_forecast(api_key, lat, lng, time=date, units="us")
{% endhighlight %}

**In [75]:**

{% highlight python %}
daily = forecast.daily()
hourly = forecast.hourly()
alerts = forecast.alerts()

for d in daily.data:
    for x in d.d:
        print x, d.d[x]

print "-----------"

for a in alerts:
    for x in d.d:
        print x, d.d[x]

print "-----------"
        
for h in hourly.data:
    if 'precipType' in h.d:
        ti = str(datetime.datetime.fromtimestamp(h.d['time']))
        pt = h.d['precipType']
        pa = h.d['precipAccumulation']
        print 
        print "%s: %f in. of %s" % (ti, pa, pt)
{% endhighlight %}

    apparentTemperatureMinTime 1422889200
    precipType snow
    cloudCover 0.41
    precipIntensityMaxTime 1422853200
    temperatureMin 16.32
    summary Partly cloudy in the afternoon.
    dewPoint 13.38
    apparentTemperatureMax 30.17
    temperatureMax 37.68
    temperatureMaxTime 1422853200
    windBearing 312
    moonPhase 0.46
    precipAccumulation 0.024
    visibility 7.87
    sunsetTime 1422917885
    pressure 1015.47
    precipProbability 0.1
    apparentTemperatureMin 0.54
    precipIntensityMax 0.0032
    icon partly-cloudy-day
    apparentTemperatureMaxTime 1422853200
    humidity 0.72
    windSpeed 11.44
    time 1422853200
    precipIntensity 0.0001
    sunriseTime 1422881121
    temperatureMinTime 1422932400
    -----------
    -----------



    ---------------------------------------------------------------------------

    KeyError                                  Traceback (most recent call last)

    <ipython-input-75-cdb1725e34d9> in <module>()
         19         ti = str(datetime.datetime.fromtimestamp(h.d['time']))
         20         pt = h.d['precipType']
    ---> 21         pa = h.d['precipAccumulation']
         22         print
         23         print "%s: %f in. of %s" % (ti, pa, pt)


    KeyError: 'precipAccumulation'


Now the problem is sampling. Assuming that the decision to treat the road network correlates tightly with forecasted and actual weather over an area of "large enough" interest to warrant treatment, then I want to keep track of weather conditions across all counties uniformly. So two questions:
* What is "large enough?"
* Based on that value, how should I sample each county?

How about this? To keep things simple, use the average size of a county to dictate sampling. Then I can simply build a grid in lat/lng space and get weather values on each grid point across the entire state. The assumption is that this grid, on average, samples each county uniformly and thus the weather as well.

To tackle the above, I loaded the GADM US admin0-2 shapes into PSQL (stored under ODOT db).
```sql
select min(st_area(geom)), max(st_area(geom)), avg(st_area(geom)), stddev(st_area(geom)) from usa_adm2 where name_1='Ohio';
```
* min: 0.065
* max: 1.407
* avg: 0.143
* std: 0.138

So unfortunately the land area of each county varies quite a lot. What about a bounding box for the extent of each county?
```sql
select st_extent(geom) from usa_adm2 where name_1='Ohio' and name_2='Allen';

BOX(-84.3975677490234 40.6426620483398,-83.8797378540038 40.920711517334)
```

That is indeed correct for the lat(min,max) and lng(min,max). So in a single bounding box, create a uniform sampling of AxB grid. Let's say every... I don't know. [The average thunderstorm has a diameter of 15mi](http://www.nssl.noaa.gov/primer/tstorm/tst_basics.html) (or 24.1402 km), so that sounds good.

Logic block is then:
* for every county
    * get extent
    * iterate over lat from lat_min to lat_max adding steps of 25 km
        * iterate over lng from lng_min to lng_max adding steps of 25 km
            * append (x,y) to coords[]
* for every coord in coords
    * for every date
        * query forecastio
            * for every attribute in weather data
                * store into a dict
* build dataframe from dict
* send to PSQL

Major consideration here: data size. There are 88 counties in OH. The snow/ice season runs from Nov 10th-ish to Mar 10th-ish, or approx 120 days, or 2928 hours. If I keep hourly data for every county, that's 260K points. If there are 10 sampling locations per county, that's 2.6M points. The calls to the forecastio API are per day per (lat/lng), so that's 106K requests (at least). I get 1K req/day for free, so to do it for free will take 106 days. Otherwise, calls are 0.0001 each. I'd pay for 105K calls, or 10.50. Even at 10x the points, this looks reasonable (105).

In the extreme, I'd use 11/1 to 4/1 (153d --> 3672h) and have avg of 20 pts/county. So, sample the space first to determine number of points in grid.

**In [49]:**

{% highlight python %}
# get the bounding box of ohio

# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')
conn = engine.connect()

# get ohio's extent
min_lng = [x for x in conn.execute("select st_xmin(st_extent(geom)) from usa_adm1 where name_1='Ohio';")][0][0]
max_lng = [x for x in conn.execute("select st_xmax(st_extent(geom)) from usa_adm1 where name_1='Ohio';")][0][0]
min_lat = [x for x in conn.execute("select st_ymin(st_extent(geom)) from usa_adm1 where name_1='Ohio';")][0][0]
max_lat = [x for x in conn.execute("select st_ymax(st_extent(geom)) from usa_adm1 where name_1='Ohio';")][0][0]

print "Min(lat,lng): ", min_lat, min_lng
print "Max(lat,lng): ", max_lat, max_lng

# clean-up the engine
conn.close()
engine.dispose()
{% endhighlight %}

    Min(lat,lng):  38.4025001526 -84.8199310303
    Max(lat,lng):  42.9616279602 -78.853012085


Turns out the GADM shapes include waterway rights boundaries, so Ohio technically extends all the way to Buffalo. This screws up the bounding box method using Ohio. Fuck it, just hardcode the upper-right for now:
41.977296, -80.519410. Note thatt, latitude: 1 deg = 110.574 km, longitude: 1 deg = 111.320*cos(latitude) km

**In [50]:**

{% highlight python %}
# cover ohio in a grid
state = 'Ohio'
max_lng = -80.519410 # fudge it
max_lat = 41.977296  # fudge it

# start and end dates
dstr = datetime.datetime(2014, 11, 1)
dend = datetime.datetime(2015, 4, 1)

# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')
conn = engine.connect()

# iterate over lat, lng bounds
grid = []
spacer = 24.0/2.
step_lat = spacer/110.574
clat = min_lat
snum = 0
while clat <= max_lat:
    clng = min_lng
    step_lng = spacer/(111.320*cos(radians(clat)))
    while clng <= max_lng:
        query = '''
                select st_intersects(st_setsrid(st_makepoint({0}, {1}), 4326), geom)
                from usa_adm1 where name_1='{2}';
                '''
        check = [x for x in conn.execute(query.format(clng, clat, state))][0][0]
        if check:
            grid.append(('fio'+str(snum), clat, clng))
            snum += 1
        clng = clng + step_lng
    clat = clat + step_lat
    
# clean-up the engine
conn.close()
engine.dispose()

# cost for fio
gsize = len(grid) # size of grid
d0 = datetime.date(2014, 11, 1)
d1 = datetime.date(2015, 4, 1)
delta = d0 - d1
days = abs(delta.days)
dpnts = days*24*gsize
calls = days*gsize
cost = (calls-1000)*0.0001
print "Grid size:", gsize
print "API calls:", calls
print '{} {:.2f}'.format("Calls cost:", cost)
print '{} {:.4e}'.format("Data points:", dpnts)
{% endhighlight %}

    Grid size: 774
    API calls: 116874
    Calls cost: 11.59
    Data points: 2.8050e+06


**In [None]:**

{% highlight python %}
# add all the fio sampling points to psql
# make a df of the samplings
fios = pd.DataFrame(grid, columns=['sampling','lat','lng'])

# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')
conn = engine.connect()

# write the df to psql
tname = 'fio_sampling'
fios.to_sql(tname, engine, if_exists='replace', index=False)

# add a geom col and index to new table and 
query = ["SELECT AddGeometryColumn ('public', '{0}', 'geom', 4326, 'POINT', 2);"]
query.append("UPDATE {0} SET geom = ST_SetSRID(ST_MakePoint(lng, lat), 4326);")
query.append("CREATE INDEX {0}_geom_idx ON public.{0} USING gist(geom);")

# add the county name in which this sampling lies
query.append("ALTER TABLE fio_sampling ADD COLUMN county text DEFAULT '';")
query.append("UPDATE {0} AS s SET county=c.name_2 FROM usa_adm2 c WHERE st_intersects(c.geom, s.geom);")

# run the queries
for q in query:
    conn.execute(q.format(tname))
    
# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

What about the sampling rate per county, i.e. are we seeing too much/little data from an area?
```sql
select avg(srate), stddev(srate) from (select c.gid as gid, c.name_2 as name, coalesce(count(s.sampling),0)/st_area(c.geom) as srate from usa_adm2 c left join fio_sampling s on st_intersects(c.geom, s.geom) where c.name_1='Ohio' group by 1,2);
```
Avg: 64.4 +/- 11.1

Only counties outside 2-sigma are:
```sql
    name    |      srate       
------------+------------------
 Lake Erie  | 31.2832998026624
 Morgan     | 87.1923348516827
```

Lake Erie is an artifact of the shapefile, so this is fine for the course sampling in lat/lng. Let's scale this shit up and scrape some data...

**In [27]:**

{% highlight python %}
# timeframe
datadir = '/home/kcavagnolo/odot/data/fio/'

# iter over each date
tdate = dstr
while tdate <= dend:

    # status and checks
    print '{} {:%m-%d-%Y}'.format("Fetching data for:", tdate)
    ofile = datadir+'{:%Y%m%d}'.format(tdate)+".csv"
    if os.path.isfile(ofile):
        print '{}'.format("Already have data for this date: skipping.")
        tdate += datetime.timedelta(days=1)
        continue
    
    # initialize a dict of all vals in FIO
    fio_idx = []
    fio_data = {}
    attributes = ['time', 'summary', 'icon', 'sunriseTime',
                  'sunsetTime', 'moonPhase', 'nearestStormDistance',
                  'nearestStormBearing', 'precipIntensity', 'precipIntensityMax',
                  'precipIntensityMaxTime', 'precipProbability', 'precipType',
                  'precipAccumulation', 'temperature', 'apparentTemperature',               
                  'dewPoint', 'windSpeed', 'windBearing', 'cloudCover',
                  'humidity', 'pressure', 'visibility', 'ozone', 'temperatureMin',
                  'temperatureMinTime', 'temperatureMax', 'temperatureMaxTime',
                  'apparentTemperatureMin', 'apparentTemperatureMinTime',
                  'apparentTemperatureMax', 'apparentTemperatureMaxTime']
    for attr in attributes:
        fio_data[attr] = []
    
    # iter over the grid
    for g in tqdm(grid):
    
        # set some params
        fio = g[0]
        lat = g[1]
        lng = g[2]
        
        # call the FIO api
        forecast = forecastio.load_forecast(api_key, lat, lng, time=tdate, units="us")
        h = forecast.hourly()
        d = h.data
        for p in d:
            fio_idx.append(fio)
            for attr in attributes:
                if attr in p.d:
                    fio_data[attr].append(p.d[attr])
                else:
                    fio_data[attr].append(0.0)
           
    # save to df and then to csv, just in case
    df_fio = pd.DataFrame(fio_data, index=fio_idx)
    df_fio.to_csv(ofile, encoding='utf-8')
    
    # increment date
    tdate += datetime.timedelta(days=1)
{% endhighlight %}

                                                     

    Fetching data for: 11-01-2014
    Already have data for this date: skipping.
    Fetching data for: 11-02-2014
    Already have data for this date: skipping.
    Fetching data for: 11-03-2014
    Already have data for this date: skipping.
    Fetching data for: 11-04-2014
    Already have data for this date: skipping.
    Fetching data for: 11-05-2014
    Already have data for this date: skipping.
    Fetching data for: 11-06-2014
    Already have data for this date: skipping.
    Fetching data for: 11-07-2014
    Already have data for this date: skipping.
    Fetching data for: 11-08-2014
    Already have data for this date: skipping.
    Fetching data for: 11-09-2014
    Already have data for this date: skipping.
    Fetching data for: 11-10-2014
    Already have data for this date: skipping.
    Fetching data for: 11-11-2014
    Already have data for this date: skipping.
    Fetching data for: 11-12-2014
    Already have data for this date: skipping.
    Fetching data for: 11-13-2014
    Already have data for this date: skipping.
    Fetching data for: 11-14-2014
    Already have data for this date: skipping.
    Fetching data for: 11-15-2014
    Already have data for this date: skipping.
    Fetching data for: 11-16-2014
    Already have data for this date: skipping.
    Fetching data for: 11-17-2014
    Already have data for this date: skipping.
    Fetching data for: 11-18-2014
    Already have data for this date: skipping.
    Fetching data for: 11-19-2014
    Already have data for this date: skipping.
    Fetching data for: 11-20-2014
    Already have data for this date: skipping.
    Fetching data for: 11-21-2014
    Already have data for this date: skipping.
    Fetching data for: 11-22-2014
    Already have data for this date: skipping.
    Fetching data for: 11-23-2014
    Already have data for this date: skipping.
    Fetching data for: 11-24-2014
    Already have data for this date: skipping.
    Fetching data for: 11-25-2014
    Already have data for this date: skipping.
    Fetching data for: 11-26-2014
    Already have data for this date: skipping.
    Fetching data for: 11-27-2014
    Already have data for this date: skipping.
    Fetching data for: 11-28-2014
    Already have data for this date: skipping.
    Fetching data for: 11-29-2014
    Already have data for this date: skipping.
    Fetching data for: 11-30-2014
    Already have data for this date: skipping.
    Fetching data for: 12-01-2014
    Already have data for this date: skipping.
    Fetching data for: 12-02-2014
    Already have data for this date: skipping.
    Fetching data for: 12-03-2014
    Already have data for this date: skipping.
    Fetching data for: 12-04-2014
    Already have data for this date: skipping.
    Fetching data for: 12-05-2014
    Already have data for this date: skipping.
    Fetching data for: 12-06-2014
    Already have data for this date: skipping.
    Fetching data for: 12-07-2014
    Already have data for this date: skipping.
    Fetching data for: 12-08-2014
    Already have data for this date: skipping.
    Fetching data for: 12-09-2014
    Already have data for this date: skipping.
    Fetching data for: 12-10-2014
    Already have data for this date: skipping.
    Fetching data for: 12-11-2014
    Already have data for this date: skipping.
    Fetching data for: 12-12-2014
    Already have data for this date: skipping.
    Fetching data for: 12-13-2014
    Already have data for this date: skipping.
    Fetching data for: 12-14-2014
    Already have data for this date: skipping.
    Fetching data for: 12-15-2014
    Already have data for this date: skipping.
    Fetching data for: 12-16-2014
    Already have data for this date: skipping.
    Fetching data for: 12-17-2014
    Already have data for this date: skipping.
    Fetching data for: 12-18-2014
    Already have data for this date: skipping.
    Fetching data for: 12-19-2014
    Already have data for this date: skipping.
    Fetching data for: 12-20-2014
    Already have data for this date: skipping.
    Fetching data for: 12-21-2014
    Already have data for this date: skipping.
    Fetching data for: 12-22-2014
    Already have data for this date: skipping.
    Fetching data for: 12-23-2014
    Already have data for this date: skipping.
    Fetching data for: 12-24-2014
    Already have data for this date: skipping.
    Fetching data for: 12-25-2014
    Already have data for this date: skipping.
    Fetching data for: 12-26-2014
    Already have data for this date: skipping.
    Fetching data for: 12-27-2014
    Already have data for this date: skipping.
    Fetching data for: 12-28-2014
    Already have data for this date: skipping.
    Fetching data for: 12-29-2014
    Already have data for this date: skipping.
    Fetching data for: 12-30-2014
    Already have data for this date: skipping.
    Fetching data for: 12-31-2014
    Already have data for this date: skipping.
    Fetching data for: 01-01-2015
    Already have data for this date: skipping.
    Fetching data for: 01-02-2015
    Already have data for this date: skipping.
    Fetching data for: 01-03-2015
    Already have data for this date: skipping.
    Fetching data for: 01-04-2015
    Already have data for this date: skipping.
    Fetching data for: 01-05-2015
    Already have data for this date: skipping.
    Fetching data for: 01-06-2015
    Already have data for this date: skipping.
    Fetching data for: 01-07-2015
    Already have data for this date: skipping.
    Fetching data for: 01-08-2015
    Already have data for this date: skipping.
    Fetching data for: 01-09-2015
    Already have data for this date: skipping.
    Fetching data for: 01-10-2015
    Already have data for this date: skipping.
    Fetching data for: 01-11-2015
    Already have data for this date: skipping.
    Fetching data for: 01-12-2015
    Already have data for this date: skipping.
    Fetching data for: 01-13-2015
    Already have data for this date: skipping.
    Fetching data for: 01-14-2015
    Already have data for this date: skipping.
    Fetching data for: 01-15-2015
    Already have data for this date: skipping.
    Fetching data for: 01-16-2015
    Already have data for this date: skipping.
    Fetching data for: 01-17-2015
    Already have data for this date: skipping.
    Fetching data for: 01-18-2015
    Already have data for this date: skipping.
    Fetching data for: 01-19-2015
    Already have data for this date: skipping.
    Fetching data for: 01-20-2015
    Already have data for this date: skipping.
    Fetching data for: 01-21-2015
    Already have data for this date: skipping.
    Fetching data for: 01-22-2015
    Already have data for this date: skipping.
    Fetching data for: 01-23-2015
    Already have data for this date: skipping.
    Fetching data for: 01-24-2015
    Already have data for this date: skipping.
    Fetching data for: 01-25-2015
    Already have data for this date: skipping.
    Fetching data for: 01-26-2015
    Already have data for this date: skipping.
    Fetching data for: 01-27-2015
    Already have data for this date: skipping.
    Fetching data for: 01-28-2015
    Already have data for this date: skipping.
    Fetching data for: 01-29-2015
    Already have data for this date: skipping.
    Fetching data for: 01-30-2015
    Already have data for this date: skipping.
    Fetching data for: 01-31-2015
    Already have data for this date: skipping.
    Fetching data for: 02-01-2015
    Already have data for this date: skipping.
    Fetching data for: 02-02-2015
    Already have data for this date: skipping.
    Fetching data for: 02-03-2015
    Already have data for this date: skipping.
    Fetching data for: 02-04-2015
    Already have data for this date: skipping.
    Fetching data for: 02-05-2015
    Already have data for this date: skipping.
    Fetching data for: 02-06-2015
    Already have data for this date: skipping.
    Fetching data for: 02-07-2015
    Already have data for this date: skipping.
    Fetching data for: 02-08-2015
    Already have data for this date: skipping.
    Fetching data for: 02-09-2015
    Already have data for this date: skipping.
    Fetching data for: 02-10-2015
    Already have data for this date: skipping.
    Fetching data for: 02-11-2015
    Already have data for this date: skipping.
    Fetching data for: 02-12-2015
    Already have data for this date: skipping.
    Fetching data for: 02-13-2015
    Already have data for this date: skipping.
    Fetching data for: 02-14-2015
    Already have data for this date: skipping.
    Fetching data for: 02-15-2015
    Already have data for this date: skipping.
    Fetching data for: 02-16-2015
    Already have data for this date: skipping.
    Fetching data for: 02-17-2015
    Already have data for this date: skipping.
    Fetching data for: 02-18-2015
    Already have data for this date: skipping.
    Fetching data for: 02-19-2015
    Already have data for this date: skipping.
    Fetching data for: 02-20-2015
    Already have data for this date: skipping.
    Fetching data for: 02-21-2015
    Already have data for this date: skipping.
    Fetching data for: 02-22-2015
    Already have data for this date: skipping.
    Fetching data for: 02-23-2015
    Already have data for this date: skipping.
    Fetching data for: 02-24-2015
    Already have data for this date: skipping.
    Fetching data for: 02-25-2015
    Already have data for this date: skipping.
    Fetching data for: 02-26-2015
    Already have data for this date: skipping.
    Fetching data for: 02-27-2015
    Already have data for this date: skipping.
    Fetching data for: 02-28-2015
    Already have data for this date: skipping.
    Fetching data for: 03-01-2015
    Already have data for this date: skipping.
    Fetching data for: 03-02-2015
    Already have data for this date: skipping.
    Fetching data for: 03-03-2015
    Already have data for this date: skipping.
    Fetching data for: 03-04-2015
    Already have data for this date: skipping.
    Fetching data for: 03-05-2015
    Already have data for this date: skipping.
    Fetching data for: 03-06-2015
    Already have data for this date: skipping.
    Fetching data for: 03-07-2015
    Already have data for this date: skipping.
    Fetching data for: 03-08-2015
    Already have data for this date: skipping.
    Fetching data for: 03-09-2015
    Fetching data for: 03-10-2015

                                                     

    
    Fetching data for: 03-11-2015

                                                     

    
    Fetching data for: 03-12-2015

                                                     

    
    Fetching data for: 03-13-2015

                                                     

    
    Fetching data for: 03-14-2015

                                                     

    
    Fetching data for: 03-15-2015

                                                     

    
    Fetching data for: 03-16-2015

                                                     

    
    Fetching data for: 03-17-2015

                                                     

    
    Fetching data for: 03-18-2015

                                                     

    
    Fetching data for: 03-19-2015

                                                     

    
    Fetching data for: 03-20-2015

                                                     

    
    Fetching data for: 03-21-2015

                                                     

    
    Fetching data for: 03-22-2015

                                                     

    
    Fetching data for: 03-23-2015

                                                     

    
    Fetching data for: 03-24-2015

                                                     

    
    Fetching data for: 03-25-2015

                                                     

    
    Fetching data for: 03-26-2015

                                                     

    
    Fetching data for: 03-27-2015

                                                     

    
    Fetching data for: 03-28-2015

                                                     

    
    Fetching data for: 03-29-2015

                                                     

    
    Fetching data for: 03-30-2015

                                                     

    
    Fetching data for: 03-31-2015

                                                     

    
    Fetching data for: 04-01-2015

                                                     

    


    

Paranoia meant I saved everything above to CSV's, and now need those in PSQL. Below checks file sizes and moves them into PSQL.

**In [28]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')

# table to build
datadir = '/home/kcavagnolo/odot/data/fio/'
tname = 'fio_weather'

# get all files
import glob
files = sorted(glob.glob(datadir+'*.csv'))
n = 0
for f in files:
    nlines = sum(1 for line in open(f))
    nlines -= 1
    if nlines != gsize*24:
        print '{} {} {}'.format("File incorrect size:", f, nlines)
        continue

    # read files
    df = pd.read_csv(f)
    df.rename(columns={'Unnamed: 0': 'sampling'}, inplace=True)
    df.columns = map(str.lower, df.columns)
    if n < 1:
        action = 'replace'
    else:
        action = 'append'
    print '{} {} {}'.format(action, f, "to"+tname)
    df.to_sql(tname, engine, if_exists=action, index=False)
    n += 1

# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

    replace /home/kcavagnolo/odot/data/fio/20141101.csv tofio_weather
    File incorrect size: /home/kcavagnolo/odot/data/fio/20141102.csv 19350
    append /home/kcavagnolo/odot/data/fio/20141103.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141104.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141105.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141106.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141107.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141108.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141109.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141110.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141111.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141112.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141113.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141114.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141115.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141116.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141117.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141118.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141119.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141120.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141121.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141122.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141123.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141124.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141125.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141126.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141127.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141128.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141129.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141130.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141201.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141202.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141203.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141204.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141205.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141206.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141207.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141208.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141209.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141210.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141211.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141212.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141213.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141214.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141215.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141216.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141217.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141218.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141219.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141220.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141221.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141222.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141223.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141224.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141225.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141226.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141227.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141228.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141229.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141230.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20141231.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150101.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150102.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150103.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150104.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150105.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150106.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150107.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150108.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150109.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150110.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150111.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150112.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150113.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150114.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150115.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150116.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150117.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150118.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150119.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150120.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150121.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150122.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150123.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150124.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150125.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150126.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150127.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150128.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150129.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150130.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150131.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150201.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150202.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150203.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150204.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150205.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150206.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150207.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150208.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150209.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150210.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150211.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150212.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150213.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150214.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150215.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150216.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150217.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150218.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150219.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150220.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150221.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150222.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150223.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150224.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150225.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150226.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150227.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150228.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150301.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150302.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150303.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150304.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150305.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150306.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150307.csv tofio_weather
    File incorrect size: /home/kcavagnolo/odot/data/fio/20150308.csv 17802
    append /home/kcavagnolo/odot/data/fio/20150309.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150310.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150311.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150312.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150313.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150314.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150315.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150316.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150317.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150318.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150319.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150320.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150321.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150322.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150323.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150324.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150325.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150326.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150327.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150328.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150329.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150330.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150331.csv tofio_weather
    append /home/kcavagnolo/odot/data/fio/20150401.csv tofio_weather


The 20141102.csv data has no dupes, so import it anyways.

**In [31]:**

{% highlight python %}
df = pd.read_csv('/home/kcavagnolo/odot/data/fio/20141102.csv')
df.rename(columns={'Unnamed: 0': 'sampling'}, inplace=True)
df.columns = map(str.lower, df.columns)
df.to_sql('fio_weather', engine, if_exists='append', index=False)
{% endhighlight %}

Okay, now in PSQL I have:
* fio_sampling -- sampling sites across Ohio
* fio_weather -- weather from 11/1 to 4/1 for all sites
* odot_districts -- service districts
* snow_ice_priority_routes -- plow routes
* usa_adm2 -- Ohio counties
* MEL -- material, equip, labor for ODOT in 2014-2015 snow/ice season

Let's also add the geom to each sampling and include the county name:
```sql
SELECT AddGeometryColumn ('public', 'fio_weather', 'geom', 4326, 'POINT', 2);
UPDATE fio_weather AS w SET geom=s.geom FROM fio_sampling s WHERE s.sampling=w.sampling;
CREATE INDEX fio_weather_geom_idx ON public.fio_weather USING gist(geom);

ALTER TABLE fio_weather ADD COLUMN county text DEFAULT '';
UPDATE fio_weather AS w SET county=c.name_2 FROM usa_adm2 c WHERE st_intersects(c.geom, w.geom);

```

## Question: what's a weather event?

ODOT has their own definition based on tonage use, but what does material use look like as a function of weather conditions? Let's try plotting material use against precip accumulation, temp, etc.

How about a roll-up of snowfall per county per day? I have weather per hour per day per sample site. So what I want is to find the average precip over a region per hour, then sum those averages in a day per county...

```sql
drop table fio_weather_agg;
create table fio_weather_agg as
select round(sum(sub.accumulation)::numeric,4) as tot_snow,
       round(stddev(sub.accumulation)::numeric,4) as std_snow,
       sub.date as date,
       sub.county
from (
      select max(precipaccumulation) as accumulation,
             to_timestamp(time)::date as date,
             date_part('hour', to_timestamp(time)) as hour,
             county
      from fio_weather
      where preciptype='snow'
      group by 2,3,4
      order by 2,3 asc) as sub
group by 3,4
order by 3,4 asc;
```

**In [46]:**

{% highlight python %}
# Let's add the snowfall totals to the summary table:
# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')
conn = engine.connect()
query = '''
        ALTER TABLE summary DROP COLUMN IF EXISTS {0};
        ALTER TABLE summary ADD COLUMN {0} numeric DEFAULT 0.0;
        UPDATE summary AS s SET {0}=sub2.{0} FROM (
            select round(sum(sub1.accumulation)::numeric,4) as for_snow,
                   sub1.county
            from (
                  select max(precipaccumulation) as accumulation,
                         to_timestamp(time)::date as date,
                         date_part('hour', to_timestamp(time)) as hour,
                         county
                  from fio_weather
                  where preciptype='snow'
                  group by 2,3,4
                  order by 2,3 asc) as sub1
            group by 2) as sub2
        WHERE sub2.county=s.county;'''

fields = ['for_snow']
for f in fields:
    conn.execute(query.format(f))
    
# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

**In [51]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')

# create default lists
rg = pd.date_range(dstr, dend).tolist()
x0 = pd.Series(rg, name='date')
y0 = pd.Series([0] * len(x0), name='amount')
dft = pd.concat([x0, y0], axis=1)

# get top ten materials used in pre-treatment
query = '''select work_date,
                  material_master_code_desc,
                  sum(amount)
           from material
           where work_date between '{0}' and '{1}'
           and material_master_code_desc in (select sub.a
                                             from (select material_master_code_desc a,
                                                          sum(amount)
                                                   from material
                                                   group by 1
                                                   order by 2 desc
                                                   limit 10
                                                   ) as sub
                                            )
           and act_name = '{2}'
           group by 1,2
           order by 1 asc;'''
df = pd.read_sql_query(query.format(dstr, dend, 'pre-treatment'), engine)

# add values for each material
mpl_fig = plt.figure()
ax1 = mpl_fig.add_subplot(311)
#ax1.set_xlabel('Date')
ax1.set_ylabel('Pre-Treatment')
materials = sorted(df['material_master_code_desc'].unique())
for m in materials:
    #df0 = pd.concat([x0, y0], axis=1)
    tx = df.loc[df['material_master_code_desc'] == m]['work_date'].tolist()
    ty = df.loc[df['material_master_code_desc'] == m]['sum'].tolist()
    for i, d in enumerate(tx):
        #df0.loc[df0.date == d, 'amount'] = ty[i]
        dft.loc[dft.date == d, 'amount'] += ty[i]
    #x = df0['date'].tolist()
    #y = df0['amount'].tolist()
    #ax1.plot(x, y, label=m)

# plot totals
x = dft['date'].tolist()
y = dft['amount'].tolist()
ynorm = y/max(y)
ax1.plot(x, ynorm)

# reset totals and run active treatment
dft = pd.concat([x0, y0], axis=1)
df = pd.read_sql_query(query.format(dstr, dend, 'snow and ice'), engine)

# add values for each material
ax2 = mpl_fig.add_subplot(312)
#ax2.set_xlabel('Date')
ax2.set_ylabel('Active Treatment')
materials = sorted(df['material_master_code_desc'].unique())
for m in materials:
    #df0 = pd.concat([x0, y0], axis=1)
    tx = df.loc[df['material_master_code_desc'] == m]['work_date'].tolist()
    ty = df.loc[df['material_master_code_desc'] == m]['sum'].tolist()
    for i, d in enumerate(tx):
        #df0.loc[df0.date == d, 'amount'] = ty[i]
        dft.loc[dft.date == d, 'amount'] += ty[i]
    #x = df0['date'].tolist()
    #y = df0['amount'].tolist()
    #ax1.plot(x, y, label=m)

# plot totals
x = dft['date'].tolist()
y = dft['amount'].tolist()
ynorm = y/max(y)
ax2.plot(x, ynorm)

# reset totals and run weather
dft = pd.concat([x0, y0], axis=1)
df = pd.read_sql_query('''select *
                          from fio_weather_agg
                          order by date asc;
                       ''', engine)
engine.dispose()

# add values for each county
ax3 = mpl_fig.add_subplot(313)
ax3.set_xlabel('Date')
ax3.set_ylabel('Snowfall [in/day]')
counties = sorted(df['county'].unique())
for c in counties:
    #df0 = pd.concat([x0, y0], axis=1)
    tx = df.loc[df['county'] == c]['date'].tolist()
    ty = df.loc[df['county'] == c]['tot_snow'].tolist()
    for i, d in enumerate(tx):
        #df0.loc[df0.date == d, 'amount'] = ty[i]
        dft.loc[dft.date == d, 'amount'] += ty[i]
    #x = df0['date'].tolist()
    #y = df0['amount'].tolist()
    #ax2.plot(x, y, label=c)

# plot totals
x = dft['date'].tolist()
y = dft['amount'].tolist()
ynorm = y/max(y)
ax3.plot(x, ynorm)
    
# display interactively in plotly
py.iplot_mpl(mpl_fig, strip_style=True)
{% endhighlight %}




<iframe id="igraph" scrolling="no" style="border:none;"seamless="seamless" src="https://plot.ly/~kcavagnolo/170.embed" height="525px" width="100%"></iframe>



### Issues

The snowfall totals are inconsistent with historicals from various sites (NOAA, Weatherunderground, etc.). The fio docs say that historical data will be back-filled if available, but otherwise they fall back to predicted values. This means the data is a mix of measured and predicted. Not good for modeling. Let's get the daily NOAA data for Ohio between 11/1 and 4/1.

**In [233]:**

{% highlight python %}
# read csv and fix
datadir = '/home/kcavagnolo/odot/data/noaa/'
df = pd.read_csv(datadir+'weather_data_OH_20141101_20150401.csv')
df.columns = map(str.lower, df.columns)
df.columns = [c.replace(' ', '_').replace('.','') for c in df.columns]
df['date'] = pd.to_datetime(df['date'].astype(str), format='%Y%m%d')
#df = df.replace(-9999.0, np.nan)
df['snow'] = df['snow']*0.0393701 # convert from mm to inches

# tell me what's in each df
nulls = np.count_nonzero(df.isnull())
print '{:15} {:d}'.format('Observations:', df.shape[0])
print '{:15} {:d}'.format('Features:', df.shape[1])
print '{:15} {:d}'.format('Nulls:', nulls)
if nulls > 0:
    print "\n---------------NULL's--------------------------\n"
    for c in df.columns:
        ns = df[c].isnull().sum()
        if ns > 0.5*df.shape[0]:
            print '{} {}'.format("Removing column", c)
            del(df[c])
    print "\n-----------------------------------------------\n"
{% endhighlight %}

    Observations:   41625
    Features:       171
    Nulls:          0


**In [234]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')
conn = engine.connect()
tname = 'noaa_weather'
df.to_sql(tname, engine, if_exists='replace', index=False)
query = ["SELECT AddGeometryColumn ('public', '{0}', 'geom', 4326, 'POINT', 2);"]
query.append("alter table noaa_weather rename latitude to lat;")
query.append("alter table noaa_weather rename longitude to lng;")
query.append("UPDATE {0} SET geom = ST_SetSRID(ST_MakePoint(lng, lat), 4326);")
query.append("CREATE INDEX {0}_geom_idx ON public.{0} USING gist(geom);")
query.append("ALTER TABLE {0} ADD COLUMN county text DEFAULT '';")
query.append("UPDATE {0} AS s SET county=c.name_2 FROM usa_adm2 c WHERE st_intersects(c.geom, s.geom);")
for q in query:
    conn.execute(q.format(tname))
conn.close()
engine.dispose()
{% endhighlight %}

How about a roll-up of snowfall per county per day? I have weather per hour per day per sample site. So what I want is to find the average precip over a region per hour, then sum those averages in a day per county...

```sql
drop table noaa_weather_agg;
create table noaa_weather_agg as
select round(sum(sub.avg_accum)::numeric,4) as avg_snow,
       round(sum(sub.max_accum)::numeric,4) as max_snow,
       round(sum(sub.min_accum)::numeric,4) as min_snow,
       sub.date as date,
       sub.county
from (
      select avg(snow) as avg_accum,
             max(snow) as max_accum,
             min(snow) as min_accum,
             date,
             county
      from noaa_weather
      where snow >= 0
      group by 4,5
      order by 4,5 asc) as sub
group by 4,5
order by 4,5 asc;
```

Create a table of the distinct stations:
```sql
drop table noaa_sampling;
create table noaa_sampling as
    select station, lat, lng, county, geom
    from noaa_weather
    where station in (select distinct(station) from noaa_weather)
    group by 1,2,3,4,5
    order by 1 asc;
CREATE INDEX noaa_sampling_geom_idx ON public.noaa_sampling USING gist(geom);
```

Turns out that a couple counties have no reporting stations in the NOAA data. They are Wyandot, Belmont, and Morgan. Where do these counties rank in the total spend?
```sql
select county, rank 
from (select county,
             rank() over(order by sum(direct_cost) desc)
      from material group by 1) as sub
where county in ('Wyandot','Belmont','Morgan');

 county  | rank 
---------+------
 Belmont |   16
 Morgan  |   63
 Wyandot |   71
```

**In [45]:**

{% highlight python %}
# Let's add the snowfall totals to the summary table:
# open the engine to the psql db
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')
conn = engine.connect()
query = '''
        ALTER TABLE summary DROP COLUMN IF EXISTS {0};
        ALTER TABLE summary ADD COLUMN {0} numeric DEFAULT 0.0;
        UPDATE summary AS s SET {0}=sub2.{0} FROM (
            select round(sum(sub1.avg_accum)::numeric,4) as avg_snow,
                   round(sum(sub1.max_accum)::numeric,4) as max_snow,
                   round(sum(sub1.min_accum)::numeric,4) as min_snow,
                   sub1.county
            from (
                  select avg(snow) as avg_accum,
                         max(snow) as max_accum,
                         min(snow) as min_accum,
                         date,
                         county
                  from noaa_weather
                  where snow >= 0
                  group by 4,5) as sub1
            group by 4) as sub2
        WHERE sub2.county=s.county;'''

fields = ['avg_snow', 'max_snow', 'min_snow']
for f in fields:
    conn.execute(query.format(f))
    
# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

I want to see a compariso of the forecasted and actual snowfalls

**In [58]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')

# compare forecasted and observed snowfall
# these will be skewed bc not all dates appear in the join
# and bc NaN's cannot be summed
query = '''select f.county as county,
                  sum(f.tot_snow) as forecasted,
                  sum(n.avg_snow) as observed
           from fio_weather_agg f
           join noaa_weather_agg n
               on n.date = f.date
               and n.county=f.county
           where f.date between '{0}' and '{1}'
           group by 1;'''
df = pd.read_sql_query(query.format(dstr, dend), engine)
engine.dispose()
{% endhighlight %}

**In [57]:**

{% highlight python %}
mpl_fig_bubble = plt.figure()
ax = mpl_fig_bubble.add_subplot(111)

plt.axis([0, 120, 0, 120])
plt.title('Total Snowfall Per County Nov 1, 2014 to Apr 1, 2015')
plt.xlabel('Measured [in]')
plt.ylabel('Forecasted [in]')

scatter = ax.scatter(
    df['observed'], # x
    df['forecasted'], # y
    c = df['forecasted'], # color scale
    s = np.square(df['observed']), # size scale
    linewidths = 2,
    edgecolor = 'w',
    alpha = 0.6,
    label = df['county']
)

for i_X, X in df.iterrows():
    plt.text(
        X['observed'],
        X['forecasted'],
        X['county'][0:5],
        size=10,
        horizontalalignment='center'
    )

x = range(-1000,1000)
y = x
plt.plot(x, y, label='Equality')
    
# make plotly
py.iplot_mpl(mpl_fig_bubble)#, strip_style=True)
{% endhighlight %}




<iframe id="igraph" scrolling="no" style="border:none;"seamless="seamless" src="https://plot.ly/~kcavagnolo/174.embed" height="525px" width="100%"></iframe>



If we take all expensives as a function of snowfall, then cost/snowfall is a unit of efficiency. Who's the best and worst?

**In [43]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')

# get top ten materials used in pre-treatment
query = '''select m.county,
                  m.sum/s.sum as eff_ratio
           from (select county,
                        sum(direct_cost)
                 from material
                 group by 1) m
           join (select county,
                        sum(avg_snow)
                 from noaa_weather_agg
                 group by 1
                 having sum(avg_snow) > 0) s on s.county=m.county
           order by 2 asc;'''
df = pd.read_sql_query(query, engine)
engine.dispose()
print 'max', df['county'].loc[df['eff_ratio'].idxmax()]
print 'min', df['county'].loc[df['eff_ratio'].idxmin()]
{% endhighlight %}

    max Ashland
    min Fulton


Let's compare Fulton and Ashland.

```sql
select county,
       act_name,
       material_master_code_desc,
       sum(direct_cost),
       sum(amount)
from material
where county in ('Ashland','Fulton')
group by 1,2,3
having sum(direct_cost) > 5000
order by 4 desc;

 county  |   act_name    | material_master_code_desc |     sum      |   sum    
---------+---------------+---------------------------+--------------+----------
 Ashland | snow and ice  | salt                      | 1019350.4918 | 17620.24
 Fulton  | snow and ice  | salt                      |  283536.3244 |  4307.75
 Fulton  | snow and ice  | calcium chloride, liquid  |    15964.672 |    29230
 Ashland | snow and ice  | beet heet concentrate     |    15218.364 | 10775.07
 Ashland | snow and ice  | salt brine                |     7749.036 |  89759.6
 Fulton  | pre-treatment | salt brine                |     7404.009 |   104420
 Ashland | snow and ice  | concrete, class c         |    6507.7492 |       31
 Ashland | pre-treatment | salt brine                |     6365.145 |    78050
```

Ashland spends substantially more on salt. Why? Lane miles, maybe?
```sql
select county,
       route_prio,
       sum(segment_le)
from snow_ice_priority_routes
where county in ('FULTON','ASHLAND')
group by 1,2
order by 2 asc;

 county  | route_prio |  sum  
---------+------------+-------
 FULTON  |          1 | 79.97
 ASHLAND |          1 | 77.24
 FULTON  |          2 | 15.72
 ASHLAND |          2 | 89.76
 ASHLAND |          3 | 86.71
 FULTON  |          3 | 40.06
```

Ashland has substantially more lane miles. Let's adjust the totals by this.

**In [59]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:zonar@localhost:5432/odot')

# get top ten materials used in pre-treatment
query = '''select initcap(lower(m.county)) as county,
                  m.sum/s.sum/r.sum as eff_ratio,
                  rank() over (order by m.sum/s.sum/r.sum asc)
           from (select lower(county) as county,
                        sum(direct_cost)
                 from material
                 group by 1) m
           join (select lower(county) as county,
                        sum(avg_snow)
                 from noaa_weather_agg
                 group by 1
                 having sum(avg_snow) > 0) s on s.county=m.county
           join (select lower(county) as county,
                        sum(segment_le)
                 from snow_ice_priority_routes
                 group by 1) r on r.county=m.county
           order by 2 asc;'''
df = pd.read_sql_query(query, engine)
engine.dispose()
print 'max', df['county'].loc[df['eff_ratio'].idxmax()]
print 'min', df['county'].loc[df['eff_ratio'].idxmin()]
{% endhighlight %}

    max Cuyahoga
    min Adams


Let's compare Fulton and Ashland.
```sql
select county,
       act_name,
       material_master_code_desc,
       sum(direct_cost),
       sum(amount)
from material
where county in ('Adams','Cuyahoga')
group by 1,2,3
having sum(direct_cost) > 5000
order by 4 desc;

  county  |   act_name   | material_master_code_desc |       sum        |   sum    
----------+--------------+---------------------------+------------------+----------
 Cuyahoga | snow and ice | salt                      |          3309231 |    67699
 Adams    | snow and ice | salt                      |           330113 |     4818
 Cuyahoga | snow and ice | calcium chloride, liquid  |           101513 |   162686
 Adams    | snow and ice | calcium chloride, liquid  |            10941 |    14596

select county,
       sum(segment_le)
from snow_ice_priority_routes
where county in ('ADAMS','CUYAHOGA')
group by 1  
order by 2 asc;

  county  |  sum   
----------+--------
 CUYAHOGA | 150.34
 ADAMS    | 216.12
```

Let's add this efficiency value to the summary table:
```sql
ALTER TABLE summary DROP COLUMN IF EXISTS eff_ratio;
ALTER TABLE summary ADD COLUMN eff_ratio numeric DEFAULT 0.0;
UPDATE summary AS s SET eff_ratio=sub1.eff_ratio FROM (
        select county,
               total_sum/lane_miles/avg_snow as eff_ratio
        from summary
        where avg_snow > 0) as sub1
    WHERE sub1.county=s.county;
    
ALTER TABLE summary DROP COLUMN IF EXISTS eff_rank;
ALTER TABLE summary ADD COLUMN eff_rank int DEFAULT NULL;
UPDATE summary AS s SET eff_rank=sub1.rank FROM (
        select county,
               rank() over (order by eff_ratio asc)
        from summary
        where eff_ratio is not NULL) as sub1
    WHERE sub1.county=s.county;
```

**In [None]:**

{% highlight python %}

{% endhighlight %}
