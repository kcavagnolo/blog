---
layout: post
title: "a"
author: Ken Cavagnolo
category : ipynotebooks
tags: [python, notebook, jupyter]
comments: true
tweets: true
---
# General notes and thoughts on Nike Geolocation Project

#Craig's Thinking
* Craig's notes are in the 'project_notes.Rmd' file
* Starts with aggregated, geolocated data and not the raw GPS
* He used nearest neighbor approach with the assumption that proclivity for a location indicates home area
* This assumption fails for sparse data, large home area, superusers, and areas with diverse environs

#Ken's Thinking

## Goals
Need to have the below values for all unique activities in the GPS data set:
* Country
* Region
* DMA
* City

Why? Need to match the geo outputs found in raw web hits agg table to those in the GPS table(s).

Problems:
* [-] region is nebulously defined; need to see method from Omniture/Axciom (Digital Envoy)
* [X] country is country
* [-] dma is dma, but not in GADM and needs to be assigned
* [-] city is city, but  undefined; get from Omniture/Axciom

## Possible Solutions

### Geocoder
* Raw data is in the form of [lat, lon] tuples
* Need to get into [country, region, dma, city] tuples
* This can then be coded to city... but how? Need some named entity to associate with, e.g. TIGER geocoding
* TIGER geocoder only works in PSQL and not Hive which is where everything lives in NGAP
* Conversion does not look feasible since we have inoperable technologies

### Nearest Neighbor

Let's try using Postgresql + PostGIS to create a single dataset of country+region+dma+city
* Found some incredibly useful datasets:
    * [Worldwide region shapes](http://www.gadm.org/)
    * [Worldwide city lat/lon](http://download.geonames.org/export/dump/)
    * [Worldwide postal codes](http://download.geonames.org/export/zip/)
    * [US DMA's](http://geocommons.com/overlays/463399)
* Load this data into PSQL w/ PostGIS extended tables
* Could use magic <-> operator in PGIS to associate a GPS point with the nearest city
* This is the poorman's "geocoding" method
* Let's try something crazy and put a 2Gb data set into PSQL

```shell
dropdb gadm
createdb gadm
psql -d gadm -c "CREATE EXTENSION postgis;"
time shp2pgsql -I -s 4326 -W "LATIN1" gadm2.shp gadm > gadm.sql
> 27.087u 10.639s 0:51.82 72.7%   0+0k 2927416+4290352io 2pf+0w

time psql -d gadm -q -f gadm.sql
> 12.054u 20.786s 2:30.57 21.8%   0+0k 4277936+1792io 0pf+0w
```

Actually doesn't take long to run (2.5 min). NB: to convert geojson to shp...
```shell
ogr2ogr -f GeoJSON tl_2014_13_tract.geojson tl_2014_13_tract.shp
```
took about 10s for an input 21Mb shp file and 35Mb geojson output.

Let's see what happens when I add the Geonames city and postal code data sets into the GADM db.

```shell
# get data
wget http://download.geonames.org/export/zip/allCountries.zip
mv allCountries.zip /home/kcavagnolo/nike_geoloc/maps/geonames/postal_codes
wget http://download.geonames.org/export/dump/allCountries.zip
mv allCountries.zip /home/kcavagnolo/nike_geoloc/maps/geonames/cities

# unpack data, clean it, create tables, add geom, create NN func
sh load_geo_data.sh

# get into db
psql -d gadm
```

```sql
# test queries
select gid, name_0, name_1, name_2 from gadm where name_0 = 'United States' and name_1 = 'Georgia';
  gid   |    name_0     | name_1  |  name_2  
--------+---------------+---------+----------
 205793 | United States | Georgia | Morgan
 205794 | United States | Georgia | Carroll
 205797 | United States | Georgia | Douglas

SELECT name, latitude, longitude FROM cities ORDER BY geom <-> st_setsrid(st_makepoint(-90,40),4326) LIMIT 5;
     name      | latitude | longitude 
---------------+----------+-----------
 Springfield   | 39.80172 | -89.64371
 JeffersonCity |  38.5767 | -92.17352
 Madison       | 43.07305 | -89.40123
 Indianapolis  | 39.76838 | -86.15804
 DesMoines     | 41.60054 | -93.60911
```

What about running NN on cities and postal codes:
```sql
SELECT
DISTINCT ON(g1.geonameid) g1.geonameid AS gref_gid,
g1.name,
g1.country_code,
g1.admin1_code,
g1.admin2_code,
g2.postal_code
FROM cities AS g1, postal AS g2
WHERE ST_DWithin(g1.geom, g2.geom, 1000)   
ORDER BY g1.geonameid, ST_Distance(g1.geom,g2.geom)
LIMIT 10;
```

Don't forget to build spatial indices for all tables and vacuum analyze:
```sql
CREATE INDEX [tablename]_geom_gist ON [tablename] USING GIST ( [geometrycolumn] );
VACUUM ANALYZE [tablename] ( [geometrycolumn] );
```

```shell
# pre index
time psql -q -d gadm -c "SELECT c.name, c.latitude, c.longitude, c.country_code, g.name_0, g.name_1, g.name_2 FROM cities c, gadm g WHERE ST_Within(c.geom, g.geom) and c.country_code = 'US';"
> 0.978u 0.158s 7:39.22 0.2%      0+0k 5232+0io 37pf+0w

# post index
time psql -q -d gadm -c "SELECT c.name, c.latitude, c.longitude, c.country_code, g.name_0, g.name_1, g.name_2 FROM cities c, gadm g WHERE ST_Within(c.geom, g.geom) and c.country_code = 'US';"
> 1.144u 0.177s 0:57.58 2.2%      0+0k 792+0io 0pf+0w
```

No shit, a speed-up of ~100x. Crazy time...
```shell
time psql -q -d gadm -c "SELECT c.name, c.latitude, c.longitude, c.country_code, g.name_0, g.name_1, g.name_2 FROM cities c, gadm g WHERE ST_Within(c.geom, g.geom);"
> 19.190u 4.539s 1:02:14.46 0.6%  0+0k 2392+0io 8pf+0w
```

Approximately an hour to complete on my laptop, that's not terrible. Let's build a new table with all this info:
```shell
time psql -q -d gadm -c "CREATE TABLE geocode as SELECT c.geonameid, c.name, c.latitude, c.longitude, c.feature_class, c.feature_code, c.country_code, c.admin1_code, c.admin2_code, c.admin3_code, c.admin4_code, c.population, c.elevation, c.dem, c.timezone, c.mod_date, g.gid, g.objectid, g.id_0, g.iso, g.name_0, g.id_1, g.name_1, g.nl_name_1, g.hasc_1, g.cc_1, g.type_1, g.engtype_1, g.validfr_1, g.validto_1, g.remarks_1, g.id_2, g.name_2, g.nl_name_2, g.hasc_2, g.cc_2, g.type_2, g.engtype_2, g.validfr_2, g.validto_2, g.remarks_2, g.id_3, g.name_3, g.nl_name_3, g.hasc_3, g.type_3, g.engtype_3, g.validfr_3, g.validto_3, g.remarks_3, g.id_4, g.name_4, g.type4, g.engtype4, g.type_4, g.engtype_4, g.validfr_4, g.validto_4, g.remarks_4, g.id_5, g.name_5, g.type_5, g.engtype_5 FROM cities c, gadm g WHERE ST_Within(c.geom, g.geom);"
>0.064u 0.036s 56:21.02 0.0%     0+0k 1704+0io 9pf+0w
```

Dump table to csv's for future use:
```shell
time psql -q -d gadm -c "COPY (select geonameid, latitude, longitude, name as city, name_0 as country, name_1 as region, name_2 as admin_reg from geocode) TO '/tmp/geocode_all.csv' DELIMITER ',' CSV HEADER;"
>0.022u 0.028s 3:02.34 0.0%      0+0k 2000+0io 91pf+0w

time psql -q -d gadm -c "COPY (select geonameid, latitude, longitude, name as city, name_0 as country, name_1 as region, name_2 as admin_reg from geocode where population > 10000) TO '/tmp/geocode_10k.csv' DELIMITER ',' CSV HEADER;"
>0.018u 0.106s 2:04.44 0.0%      0+0k 10392+0io 37pf+0w

time psql -q -d gadm -c "COPY (select geonameid, latitude, longitude, name as city, name_0 as country, name_1 as region, name_2 as admin_reg from geocode where population > 100000) TO '/tmp/geocode_100k.csv' DELIMITER ',' CSV HEADER;"
>0.006u 0.086s 1:48.35 0.0%      0+0k 16368+0io 194pf+0w
```

Make a subsample of the full csv if needed:
```shell
head -n10000 ../data/geocode.csv > ../data/geo_test.csv
```

Produces a master table (geocode) that has every city in the geonames data set assigned to country, region, admin. So, this is kind of what we needed in the first place. Table is 10's of Gb, but dropping g.geom col reduces to a few Gb. Total of ~4M entires in csv, which is ~1Gb.

But, how to take the activities in NGAP and assign each to a single record in this table? Can't do it in PSQL, why?
* Nike hates PSQL
* PSQL not in NGAP anyways
* Would be impossibly slow (90M+ unique acitivies versus ____ records in the geocode table)

So, need to get the master dataset creation into Hive (see below) and then run big calculation in Spark.

#### Option A
* Read activity data out of Hive table into RDD (test set in data/activity_agg_dec.csv)
* Read geocode table into RDD
* Broadcast geocode data to workers
* Build function to calculate NN:
    * filter GADM points to be within 0.1 degree (1 deg ~ 110 km, so 10 km) of activity avg lat/lon
    * calc euclidean distance between all remaining points
    * select GADM point with min distance
    * return geonameid of this point and all its attributes

**In [None]:**

{% highlight python %}
# make some fake gps data first
from faker import Factory
faker = Factory.create()
f = open('../data/gps_fake.csv','w')
for i in range(0,50000):
    f.write(("%s,%f,%f\n") % (faker.city(), faker.latitude(), faker.longitude()))
f.close()
{% endhighlight %}
# copy data over into hdfs
hdfs dfs -mkdir /user/kcavag/geolocation/data/test
hdfs dfs -copyFromLocal /home/kcavag/nike_geoloc/data/* /user/kcavag/geolocation/data/test
**In [10]:**

{% highlight python %}
# setup the spark contexts and libs
from pyspark import SparkContext, SparkConf
from pyspark.sql import HiveContext, SQLContext
from datetime import datetime
from operator import itemgetter
from math import radians, cos, sin, asin, sqrt

jar_path = '/usr/local/hadoop/gis/gis-tools-hadoop-2.0.jar' 
spark_config = SparkConf().setMaster('local').setAppName('geoloc').set("spark.jars", jar_path) 
sc = SparkContext(conf=spark_config) 
hc = HiveContext(sc)
{% endhighlight %}

**In [2]:**

{% highlight python %}
# read in the geocode data1
geo_file = sc.textFile("../data/geocode_10k.csv", minPartitions=16)
geo_rdd = (geo_file
           .map(lambda x: x.split(","))
           .cache())

# drop the header
#header = geo_rdd.first()
#geo_rdd = geo_rdd.filter(lambda line: line != header)

# convert rdd to flat list and broadcast
geos = sc.broadcast(geo_rdd.collect())

# check file size
print("%i cities in geo file" % (geo_rdd.count()))
{% endhighlight %}

    30721 cities in geo file


**In [3]:**

{% highlight python %}
# read fake gps data into rdd
gps_file = sc.textFile("../data/gps_fake.csv", minPartitions=16)
gps_rdd = (gps_file
           .map(lambda x: x.split(","))
           .cache())
print("%i GPS points in geo file" % (gps_rdd.count()))
{% endhighlight %}

    50000 GPS points in geo file


**In [None]:**

{% highlight python %}
# testing speed of the filtering
lat1, lon1 = 28.215, 40.110
nns = [x for x in geos.value if (abs(lon2-lon1) < 0.5 and abs(lat2-lat1) < 0.5)]
strtime = datetime.utcnow()
for g in nns:
    lon2, lat2 = float(g[2]), float(g[1])
    lon2, lat2 = map(radians, [lon2, lat2])
    dlon = lon2 - lon1 
    dlat = lat2 - lat1 
    a = sin(dlat/2.)**2. + cos(lat1) * cos(lat2) * sin(dlon/2.)**2.
    c = 2. * asin(sqrt(a)) 
    km = 6367. * c
interval = str(datetime.utcnow()-strtime)
print("Runtime: %s" % (interval))
{% endhighlight %}

**In [5]:**

{% highlight python %}
# setup the haversine equation using broadcast
def haversine(pnt):
    lon1, lat1 = float(pnt[2]), float(pnt[1])
    nns = [x for x in geos.value if (abs(float(x[2])-lon1) < 0.1 and abs(float(x[1])-lat1) < 0.1)]
    lon1, lat1 = map(radians, [lon1, lat1])
    dists = []
    add = dists.append
    for g in nns:
        lon2, lat2 = float(g[2]), float(g[1])
        lon2, lat2 = map(radians, [lon2, lat2])
        dlon = lon2 - lon1 
        dlat = lat2 - lat1 
        a = sin(dlat/2.)**2. + cos(lat1) * cos(lat2) * sin(dlon/2.)**2.
        c = 2. * asin(sqrt(a)) 
        km = 6367. * c
        add((g[0], km))
    if len(dists) >= 1:
        return min(dists,key=itemgetter(1))
    else:
        return ('None', -1.0)
{% endhighlight %}

**In [6]:**

{% highlight python %}
# run the calculation
strtime = datetime.utcnow()
nn = gps_rdd.map(lambda x: haversine(x)).collect()
interval = str(datetime.utcnow()-strtime)
print("Runtime: %s" % (interval))
{% endhighlight %}

    Runtime: 0:07:47.142955


**In [71]:**

{% highlight python %}
print gps_rdd.take(2)
print nn[1]

list3 = [e for e in geos.value if e[0] in nn[1]]
print list3
{% endhighlight %}

    [[u'Trantowborough', u'-49.030777', u'-31.282075'], [u'West Elnashire', u'28.394176', u'-163.416677']]
    ('None', -1.0)
    []


#### Nested RDD's

The below code doesn't work. Initial attempt was to map gps_rdd to map of haversine, but that's a nested rdd transaction which is not supported in Spark:

*"No, it is not possible, because the items of an RDD must be serializable and a RDD is not serializable. And this makes sense, otherwise you might transfer over the network a whole RDD which is a problem if it contains a lot of data. And if it does not contain a lot of data, you might and you should use an array or something like it."*

**In [None]:**

{% highlight python %}
# define a nearest neighbor function to return just the city geonameid
def find_nn(pnt, geo_rdd):
    nn = (geo_rdd
          .map(lambda x: (int(x[0]), haversine((float(x[2]), float(x[1])), (pnt[1], pnt[0]))))
          .takeOrdered(1, lambda s: s[1]))
    return nn[0][0]

alat = 33.756011
alon = -84.389018
find_nn((alat,alon), geo_rdd)
nn_rdd = gps_rdd.map(lambda x: find_nn((x[1],x[2]), geo_rdd))
nn_rdd.take(5)
{% endhighlight %}

**In [None]:**

{% highlight python %}
# try to determine the RDD types automatically
import pandas as pd
df = pd.read_csv("/tmp/test.csv", sep=',', nrows=1)
names = df.columns
types = []
for i in range(len(names)):
    tp = names[i] + " "
    if df.dtypes[i] == "O":
        tp += "STRING"
    elif df.dtypes[i] == "int64":
        tp += "INT"
    else:
        tp += "DOUBLE"
    types.append(tp)
print types
{% endhighlight %}

**In [None]:**

{% highlight python %}
# create mock hive table
table_name = 'junk'
file_name = 'junk.csv'
hc.sql('drop table if exists %s' %table_name)
hc.sql("""CREATE TABLE IF NOT EXISTS %s (%s) row format delimited fields terminated by '%s' LINES TERMINATED BY '\n' tblproperties ('skip.header.line.count'='1')""" %(table_name, ','.join(types), sep))
hc.sql("LOAD DATA LOCAL INPATH '%s' OVERWRITE INTO TABLE %s" %(file_name, table_name))

# parquet file
rdd = hc.sql("SELECT * FROM %s" %table_name)
rdd.saveAsParquetFile("%s" %table_name)
rdd = SQLContext(sc).parquetFile("parquet_dir")
rdd.registerTempTable("mytable")
rdd.sql("select count(*) from mytable")
{% endhighlight %}

#### Option B
Alternate method (relying on existing NN method):
* Load data into Rtree in Python
* Run nearest neighbor on Spark

**In [None]:**

{% highlight python %}
from rtree import index

def build_tree(record):
    rtree.insert(record[0], (record[1], record[2]))

def nn_lookup(rtree, lat, lon):
    return list(rtree.nearest(lat, lon))[0]

# build rtree from master geocode table
rtree_idx = parts.build_tree()

# read lat, lon from gps activity table
lookup(rtree_idx, lat, lon)
{% endhighlight %}

Fail! Cannot broadcast an rtree -- it has to be built monolitically by one process. Interesting side project.

#### Creating GIS Lookups in Hive
* As is, the Shapefile and GeoJSON formats are incompatible with ESRI's GIS tooling
* Expects un/enclosed format
* Soooo.... what about just doing it myself?
* Reference links:
    * [JSON SerDe](https://github.com/Esri/spatial-framework-for-hadoop/wiki/Hive-JSON-SerDe)
    * [JSON Utils](https://github.com/Esri/spatial-framework-for-hadoop/wiki/JSON-Utilities)
    * [ESRI geom defs](http://help.arcgis.com/en/arcgisserver/10.0/apis/rest/geometry.html)
* Testing this out on Lyuba which has setup:
    * Linux lyuba00 2.6.32-358.el6.x86_64 #1 SMP Fri Feb 22 00:31:26 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
    * Hive 0.14.0.2.2.0.0-2041
    * CentOS release 6.4 (Final)
* To reformat the geojson output by ogr2ogr, need to do the following:
    * Remove the leading fields:
        "type": "FeatureCollection",
        "features": [
    * Remove below from each feature entry:
        "type": "Feature",
    * Rename (a) to (b):
        * (a) "properties"
        * (b) "attributes"
    * Replace (a) with (b):
        * (a) "type": "Polygon", "coordinates"
        * (b) "rings"
        
#### Option A
* I don't like code below
* Uses the osgeo lib which is nice
* But creates messy json strings instead of dicts

**In [None]:**

{% highlight python %}
from osgeo import ogr
import json
infile = r'/home/kcavagnolo/nike_geoloc/maps/tiger/tl_2014_13_tract.shp'
shpdata = ogr.Open(infile)
if not shpdata:
    raise IOError('Could not open ' + infile)
layer = shpdata.GetLayer()
#for fid in range(layer.GetFeatureCount()):
fid = 0
feat = layer.GetFeature(fid)
json = feat.ExportToJson() # creates a string
json = json.replace(", ",",").replace(": ",":")
geom = feat.GetGeometryRef()
{% endhighlight %}

#### Option B
* The below code is now in scripts/shp2ujson.py
* Outputs a huge JSON that can be read into Hive
* Much sexier than the above
* But, it relies on fiona lib to run
* Treats shapefile as a dict

**In [None]:**

{% highlight python %}
import fiona
import json
infile = r'/home/kcavagnolo/nike_geoloc/maps/tiger/tl_2014_13_tract.shp'
outfile = open("test.json", "w")
with fiona.collection(infile, "r") as source:
    for feat in source:
        geometry = feat['geometry']['coordinates']
        geometry = "\"geometry\":{\"rings\":"+str(geometry).replace("(","[").replace(")","]")+"}"
        attributes = json.dumps(feat['properties'])
        attributes = "\"attributes\":"+str(attributes)
        outfile.write("{"+attributes+","+geometry+"}\n")
outfile.close()
{% endhighlight %}

Also need the Geonames dataset in Hive to run any point-in-polygon (pip) queries. Steps to this end are similar to those for PSQL above:
* Get data
* Remove non-ascii characters
* Remove fields we don't need or want
* Re-format to an unenclosed JSON
* Load into Hive

All these steps are contained in the script found at scripts/tsv2ujson.py

#### Loading the JSON into Hive
* Load jars
* Read UDF's
* Store data in HDFS
# get GIS for Hadoop tar and udf's
wget https://github.com/Esri/gis-tools-for-hadoop/releases/download/v2.0/gis-tools-hadoop-2.0.jar
wget https://github.com/Esri/gis-tools-for-hadoop/archive/v2.0.tar.gz
tar -zxvf gis-tools-for-hadoop-2.0.tar.gz

# create dir's in HDFS
hdfs dfs -mkdir /user/kcavag/
hdfs dfs -mkdir /user/kcavag/geolocation
hdfs dfs -mkdir /user/kcavag/geolocation/data
hdfs dfs -mkdir /user/kcavag/geolocation/data/tiger
hdfs dfs -mkdir /user/kcavag/geolocation/data/gadm
hdfs dfs -mkdir /user/kcavag/geolocation/data/geonames
hdfs dfs -mkdir /user/kcavag/geolocation/data/dma

# copy data over into hdfs
hdfs dfs -copyFromLocal /home/kcavag/gis-tools-for-hadoop-2.0/samples/data/* /user/kcavag/geolocation/data/
hdfs dfs -copyFromLocal /home/kcavag/tl_2014_13_tract.geojson /user/kcavag/geolocation/data/tiger/
hdfs dfs -copyFromLocal /home/kcavag/nike_geoloc/maps/gadm/gadm_hive.json /user/kcavag/geolocation/data/gadm/
hdfs dfs -copyFromLocal /home/kcavag/nike_geoloc/maps/geonames/cities/cities_hive.json /user/kcavag/geolocation/data/geonames/
hdfs dfs -copyFromLocal /home/kcavag/nike_geoloc/maps/dma/dma_hive.json /user/kcavag/geolocation/data/dma/

# add the ESRI udf's and load the functions
hive
add jar /home/kcavagnolo/gis-tools-hadoop-2.0.jar;
add jar /usr/local/hadoop/gis/gis-tools-hadoop-2.0.jar;
create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point';
create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains';
create temporary function ST_Within as 'com.esri.hadoop.hive.ST_Within';
create temporary function ST_GeomFromGeoJson as 'com.esri.hadoop.hive.ST_GeomFromGeoJson';
create temporary function ST_AsGeoJson as 'com.esri.hadoop.hive.ST_AsGeoJson';
create temporary function ST_GeometryType as 'com.esri.hadoop.hive.ST_GeometryType';
# * Upload the reformated json produced by script to NGAP
* Build the Hive table from that file
* Run some queries against the table

```sql
DROP TABLE gadm;
CREATE EXTERNAL TABLE IF NOT EXISTS gadm (OBJECTID int, ID_0 int, ISO string, NAME_0 string, ID_1 int, NAME_1 string, NL_NAME_1 string, HASC_1 string, ID_2 int, NAME_2 string, NL_NAME_2 string, HASC_2 string, ID_3 int, NAME_3 string, NL_NAME_3 string, HASC_3 string, ID_4 int, NAME_4 string, TYPE4 string, ENGTYPE4 string, ID_5 int, NAME_5 string, TYPE_5 string, ENGTYPE_5 string, Shape_Leng double, Shape_Area double, Geom binary)
ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde'
STORED AS INPUTFORMAT 'com.esri.json.hadoop.UnenclosedJsonInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/user/kcavagnolo/geolocation/data/gadm';
```

```sql
DROP TABLE geonames;
CREATE EXTERNAL TABLE IF NOT EXISTS geonames (geonameid string, name string, fclass string, fcode string, country string, admin1 string, admin2 string, admin3 string, admin4 string, population string, Geom binary)
ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde'
STORED AS INPUTFORMAT 'com.esri.json.hadoop.UnenclosedJsonInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/user/kcavagnolo/geolocation/data/geonames';
```

```sql
DROP TABLE dma;
CREATE EXTERNAL TABLE IF NOT EXISTS dma (gid int, name string, income_6 double, income_ra0 double, income_7 double, income_lo0 double, income_8 double, dma0 double, income_70 double, dma_1 string, income_60 double, income_0 double, ebitous0 double, latitude0 double, longitude0 double, descriptio string, income_30 double, income_3 double, income_20 double, income_31 double, income_120 double, income_21 double, income_5 double, incomf_4 double, income_12 double, Geom binary)
ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde'
STORED AS INPUTFORMAT 'com.esri.json.hadoop.UnenclosedJsonInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/user/kcavagnolo/geolocation/data/dma';
```

Run a cuople test queries
```sql
select objectid, iso, name_0, ST_GeometryType(Geom) from gadm limit 5;
select geonameid, name, ST_AsGeoJson(Geom) from geonames limit 5;
select gid, dma_1, ST_AsGeoJson(Geom) from dma limit 5;
select name_0, name_1, name_2, name_3, name_4, name_5, ST_AsGeoJson(Geom) from gadm limit 1;
SELECT name_0, name_1, name_2, name_3, name_4, name_5 FROM gadm WHERE ST_Contains(geom, ST_Point(-83.641432, 32.643191));
```

Try a binning exercise (by geo)
```sql
SELECT g.name_0, g.name_1, g.name_2, g.name_3, g.name_4, g.name_5, count(*) cnt FROM gadm g
JOIN earthquakes e
WHERE ST_Contains(g.geom, ST_Point(e.longitude, e.latitude))
GROUP BY g.name_1
ORDER BY cnt desc;
```

Try inverting the binning to get loc for each lat/lon
```sql
SELECT e.latitude, e.longitude, g.name_0, g.name_1, g.name_2, g.name_3 FROM earthquakes e
JOIN gadm g
WHERE ST_Within(ST_Point(e.longitude, e.latitude), g.geom);
```

The above works, which means running the below should return a single location for each activity of the form:
* name_0 = country, e.g. United States
* name_1 = region, e.g. Texas (a state in Texas)
* name_2 = admin, e.g. Rusk (a county in Texas)
```sql
SELECT *
FROM agg_gps_summary_december a
JOIN gadm g
WHERE ST_Within(ST_Point(a.avg_lon, a.avg_lat), g.geom);
```

This method does not yield a city yet. To do that, we need another technique, like nearest neighbor.

## Confidence Scoring
* Let's start with the groundtruth about a user: their GPS data
* GPS tracks are detections of a user's occupancy model
* The analog is interestingly "wildlife survey data with imperfect detectability"
* This is typically modelled using MCMC
* So we've got some sweet tools to chain together
    * GIS for Hadoop to do the geolocation
    * PyMC to do the modelling of geolocation
    * PySpark to do all this quickly

**In [None]:**

{% highlight python %}
# Occupancy data - rows are sites, with replicate surveys conducted at each site
from pylab import *
from pymc import *
from numpy import *
observations = array([[0,0,0,1,1], [0,1,0,0,0], [0,1,0,0,0], [1,1,1,1,0], [0,0,1,0,0], 
                      [0,0,1,0,0], [0,0,1,0,0], [0,0,1,0,0], [0,0,1,0,0], [1,0,0,0,0], 
                      [0,0,1,1,1], [0,0,1,1,1], [1,0,0,1,1], [1,0,1,1,0], [0,0,0,0,0], 
                      [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], 
                      [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], 
                      [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], 
                      [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], 
                      [0,0,0,1,0], [0,0,0,1,0], [0,0,0,0,1], [0,0,0,0,1]])
obs_rdd = sc.parallelize(observations, 4)
{% endhighlight %}

**In [None]:**

{% highlight python %}
# map observations to flat values
k = 5
y = obs_rdd.map(lambda x: sum(x))
z_start = y.filter(lambda x: x > 0)
{% endhighlight %}

**In [None]:**

{% highlight python %}
# Prior on probability of detection
p = Beta('p', alpha=1, beta=1, value=0.99)
 
# Prior on probability of occupancy
psi = Beta('psi', alpha=1, beta=1, value=0.01)
 
#latent states for occupancy
z = Bernoulli('z', p=psi, value=z_start, plot=False)
{% endhighlight %}

**In [None]:**

{% highlight python %}
#Number of truly occupied sites in the sample (finite-sample occupancy)
@deterministic(plot=True)
def Num_occ(z=z):
   out = sum(z)
   return out
 
#unconditional probabilities of detection at each site (zero for unoccupied sites, p for occupied sites)
@deterministic(plot=False)
def pdet(z=z, p=p):
    out = z*p
    return out
 
#likelihood
def binom_mcmc(y, k, pdet):
    Y = Binomial('Y', n=k, p=pdet, value=y, observed=True)
    return Y
{% endhighlight %}

**In [None]:**

{% highlight python %}
@distributedMCMC(plot=False)

result = y.map(lambda x: binom_mcmc(x))
{% endhighlight %}
